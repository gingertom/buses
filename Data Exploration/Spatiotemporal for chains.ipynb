{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import feather\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from tqdm import tqdm_pandas\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = feather.read_dataframe(\"../data_files/B/once/75days/stop_events_with_geo_train_test_averages_prev_next_dwell.feather\")\n",
    "se = se.set_index(se.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # And now for just segments:\n",
    "se[\"diff_segment_and_mean_by_segment_code\"] = (\n",
    "    se[\"segment_duration\"]\n",
    "    - se[\"mean_durations_by_segment_code\"]\n",
    ")\n",
    "se[\"diff_segment_and_mean_by_segment_code_and_hour_and_day\"] = (\n",
    "    se[\"segment_duration\"]\n",
    "    - se[\"mean_durations_by_segment_code_and_hour_and_day\"]\n",
    ")\n",
    "\n",
    "se[\"diff_percent_segment_and_mean_by_segment_code\"] = (\n",
    "    se[\"diff_segment_and_mean_by_segment_code\"]\n",
    "    * 100\n",
    "    / se[\"mean_durations_by_segment_code\"]\n",
    ")\n",
    "\n",
    "se[\"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\"] = (\n",
    "    se[\"diff_segment_and_mean_by_segment_code_and_hour_and_day\"]\n",
    "    * 100\n",
    "    / se[\"mean_durations_by_segment_code_and_hour_and_day\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se_min = se.iloc[0:1000, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate this from scratch as we need both test and train data.\n",
    "# ts_5 = se.pivot_table(\n",
    "#     index=\"actualArrival\",\n",
    "#     columns=\"segment_code\",\n",
    "#     values=\"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\",\n",
    "#     aggfunc=np.median,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://stackoverflow.com/questions/31661604/efficiently-create-sparse-pivot-tables-in-pandas\n",
    "\n",
    "arrival_c = CategoricalDtype(sorted(se.actualArrival.unique()), ordered=True)\n",
    "segment_code_c = CategoricalDtype(sorted(se.segment_code.unique()), ordered=True)\n",
    "\n",
    "row = se.actualArrival.astype(arrival_c).cat.codes\n",
    "col = se.segment_code.astype(segment_code_c).cat.codes\n",
    "sparse_matrix = csr_matrix((se[\"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\"], (row, col)), \\\n",
    "                           shape=(arrival_c.categories.size, segment_code_c.categories.size))\n",
    "\n",
    "# # >>> sparse_matrix\n",
    "# # <3x4 sparse matrix of type '<class 'numpy.int64'>'\n",
    "# #      with 6 stored elements in Compressed Sparse Row format>\n",
    "\n",
    "# # >>> sparse_matrix.todense()\n",
    "# # matrix([[0, 1, 0, 1],\n",
    "# #         [1, 0, 0, 1],\n",
    "# #         [1, 0, 1, 0]], dtype=int64)\n",
    "\n",
    "\n",
    "# ts = pd.SparseDataFrame(sparse_matrix, \\\n",
    "#                          index=arrival_c.categories, \\\n",
    "#                          columns=segment_code_c.categories, \\\n",
    "#                          default_fill_value=np.nan)\n",
    "# # >>> dfs\n",
    "# #         a   b   c   d\n",
    "# #  him    0   1   0   1\n",
    "# #   me    1   0   0   1\n",
    "# #  you    1   0   1   0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts.loc['2018-12-16 14:00:00':'2018-12-16 14:08:01', '1200BOB20146_1200DOY38562_0'].dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm_pandas(tqdm())\n",
    "\n",
    "# print(len(se))\n",
    "\n",
    "# def get_last_30(row):\n",
    "#     journeys = ts.loc[row['actualArrival'] - pd.Timedelta(\"30 min\"):row['actualArrival'] - pd.Timedelta(\"1 sec\"), row['segment_code']].dropna()\n",
    "    \n",
    "#     return journeys.mean(), journeys.count()\n",
    "    \n",
    "# se['last_30_mins_mean'], se['last_30_mins_count'] = zip(*se.progress_apply(get_last_30, axis=1))\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime(arrival_c.categories).slice_indexer('2018-12-16 14:00:00','2018-12-16 14:08:01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sobj.start\n",
    "# sobj.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_code_c.categories.get_loc('1200BOB20146_1200DOY38562_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_code_c.categories[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_matrix[2667278:2667464,9].data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(se))\n",
    "\n",
    "# arrival_index = pd.to_datetime(arrival_c.categories)\n",
    "# segment_code_index = segment_code_c.categories\n",
    "\n",
    "# last_15_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_15_mins_count = np.empty(se.shape[0]).astype(int)\n",
    "# last_15_30_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_15_30_mins_count = np.empty(se.shape[0]).astype(int)\n",
    "# last_30_45_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_30_45_mins_count = np.empty(se.shape[0]).astype(int)\n",
    "# last_45_60_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_45_60_mins_count = np.empty(se.shape[0]).astype(int)\n",
    "\n",
    "# def get_recent_buses(row, idx):\n",
    "    \n",
    "#     # 1800 seconds is 30 minutes, 1801 is 30 minutes and 1 second. \n",
    "#     slice_obj_15 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"15 min\"), row['actualArrival'] - pd.Timedelta(\"1 sec\"))\n",
    "#     slice_obj_15_30 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"30 min\"), row['actualArrival'] - pd.Timedelta(\"901 sec\"))\n",
    "#     slice_obj_30_45 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"45 min\"), row['actualArrival'] - pd.Timedelta(\"1801 sec\"))\n",
    "#     slice_obj_45_60 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"60 min\"), row['actualArrival'] - pd.Timedelta(\"2701 sec\"))\n",
    "    \n",
    "#     column_index = segment_code_c.categories.get_loc(row['segment_code'])\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_15,column_index].data\n",
    "    \n",
    "#     last_15_mins_mean[idx] = journeys.mean()\n",
    "#     last_15_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_15_30,column_index].data\n",
    "    \n",
    "#     last_15_30_mins_mean[idx] = journeys.mean()\n",
    "#     last_15_30_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_30_45,column_index].data\n",
    "    \n",
    "#     last_30_45_mins_mean[idx] = journeys.mean()\n",
    "#     last_30_45_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_45_60,column_index].data\n",
    "    \n",
    "#     last_45_60_mins_mean[idx] = journeys.mean()\n",
    "#     last_45_60_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "# for idx, (_, row) in tqdm(enumerate(se[['actualArrival', 'segment_code']].iterrows())):\n",
    "#     get_recent_buses(row, idx)\n",
    "    \n",
    "# se['last_15_mins_mean'] = last_15_mins_mean\n",
    "# se['last_15_mins_count'] = last_15_mins_count \n",
    "# se['last_15_30_mins_mean'] = last_15_30_mins_mean \n",
    "# se['last_15_30_mins_count'] = last_15_30_mins_count\n",
    "# se['last_30_45_mins_mean'] = last_30_45_mins_mean \n",
    "# se['last_30_45_mins_count'] = last_30_45_mins_count\n",
    "# se['last_45_60_mins_mean'] = last_45_60_mins_mean \n",
    "# se['last_45_60_mins_count'] = last_45_60_mins_count\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.figure().gca()\n",
    "# ax.hist(se['last_30_mins_count'], bins=20, range=(0,20));\n",
    "# plt.xlabel(\"Number of buses\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Number of buses in previous 30 mins\")\n",
    "\n",
    "# ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se.reset_index().to_feather(\"with_last_slots.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(se))\n",
    "\n",
    "# arrival_index = pd.to_datetime(arrival_c.categories)\n",
    "# segment_code_index = segment_code_c.categories\n",
    "\n",
    "# last_15_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_15_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "# last_15_30_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_15_30_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "# last_30_45_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_30_45_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "# last_45_60_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_45_60_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "\n",
    "# last_15_mins_mean[:] = np.nan\n",
    "# last_15_30_mins_mean[:] = np.nan\n",
    "# last_30_45_mins_mean[:] = np.nan\n",
    "# last_45_60_mins_mean[:] = np.nan\n",
    "\n",
    "\n",
    "# def get_recent_buses(row, idx):\n",
    "    \n",
    "#     # 1800 seconds is 30 minutes, 1801 is 30 minutes and 1 second. \n",
    "#     slice_obj_15 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"15 min\"), row['actualArrival'] - pd.Timedelta(\"1 sec\"))\n",
    "#     slice_obj_15_30 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"30 min\"), row['actualArrival'] - pd.Timedelta(\"901 sec\"))\n",
    "#     slice_obj_30_45 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"45 min\"), row['actualArrival'] - pd.Timedelta(\"1801 sec\"))\n",
    "#     slice_obj_45_60 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"60 min\"), row['actualArrival'] - pd.Timedelta(\"2701 sec\"))\n",
    "    \n",
    "#     try:\n",
    "#         column_index = segment_code_index.get_loc(row['prev_segment_code_1'])\n",
    "#     except KeyError:\n",
    "#         return\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_15,column_index].data\n",
    "    \n",
    "#     last_15_mins_mean[idx] = journeys.mean()\n",
    "#     last_15_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_15_30,column_index].data\n",
    "    \n",
    "#     last_15_30_mins_mean[idx] = journeys.mean()\n",
    "#     last_15_30_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_30_45,column_index].data\n",
    "    \n",
    "#     last_30_45_mins_mean[idx] = journeys.mean()\n",
    "#     last_30_45_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_45_60,column_index].data\n",
    "    \n",
    "#     last_45_60_mins_mean[idx] = journeys.mean()\n",
    "#     last_45_60_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "# for idx, (_, row) in tqdm(enumerate(se[['actualArrival', 'prev_segment_code_1']].iterrows())):\n",
    "#     get_recent_buses(row, idx)\n",
    "    \n",
    "# se['prev_segm_last_15_mins_mean'] = last_15_mins_mean\n",
    "# se['prev_segm_last_15_mins_count'] = last_15_mins_count \n",
    "# se['prev_segm_last_15_30_mins_mean'] = last_15_30_mins_mean \n",
    "# se['prev_segm_last_15_30_mins_count'] = last_15_30_mins_count\n",
    "# se['prev_segm_last_30_45_mins_mean'] = last_30_45_mins_mean \n",
    "# se['prev_segm_last_30_45_mins_count'] = last_30_45_mins_count\n",
    "# se['prev_segm_last_45_60_mins_mean'] = last_45_60_mins_mean \n",
    "# se['prev_segm_last_45_60_mins_count'] = last_45_60_mins_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(se))\n",
    "\n",
    "# arrival_index = pd.to_datetime(arrival_c.categories)\n",
    "# segment_code_index = segment_code_c.categories\n",
    "\n",
    "# last_15_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_15_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "# last_15_30_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_15_30_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "# last_30_45_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_30_45_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "# last_45_60_mins_mean = np.empty(se.shape[0]).astype(float)\n",
    "# last_45_60_mins_count = np.zeros(se.shape[0]).astype(int)\n",
    "\n",
    "# last_15_mins_mean[:] = np.nan\n",
    "# last_15_30_mins_mean[:] = np.nan\n",
    "# last_30_45_mins_mean[:] = np.nan\n",
    "# last_45_60_mins_mean[:] = np.nan\n",
    "\n",
    "# def get_recent_buses(row, idx):\n",
    "    \n",
    "#     # 1800 seconds is 30 minutes, 1801 is 30 minutes and 1 second. \n",
    "#     slice_obj_15 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"15 min\"), row['actualArrival'] - pd.Timedelta(\"1 sec\"))\n",
    "#     slice_obj_15_30 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"30 min\"), row['actualArrival'] - pd.Timedelta(\"901 sec\"))\n",
    "#     slice_obj_30_45 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"45 min\"), row['actualArrival'] - pd.Timedelta(\"1801 sec\"))\n",
    "#     slice_obj_45_60 = arrival_index.slice_indexer(row['actualArrival'] - pd.Timedelta(\"60 min\"), row['actualArrival'] - pd.Timedelta(\"2701 sec\"))\n",
    "    \n",
    "#     try:\n",
    "#         column_index = segment_code_index.get_loc(row['next_segment_code_1'])\n",
    "#     except KeyError:\n",
    "#         return\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_15,column_index].data\n",
    "    \n",
    "#     last_15_mins_mean[idx] = journeys.mean()\n",
    "#     last_15_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_15_30,column_index].data\n",
    "    \n",
    "#     last_15_30_mins_mean[idx] = journeys.mean()\n",
    "#     last_15_30_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_30_45,column_index].data\n",
    "    \n",
    "#     last_30_45_mins_mean[idx] = journeys.mean()\n",
    "#     last_30_45_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "#     journeys = sparse_matrix[slice_obj_45_60,column_index].data\n",
    "    \n",
    "#     last_45_60_mins_mean[idx] = journeys.mean()\n",
    "#     last_45_60_mins_count[idx] = journeys.shape[0]\n",
    "    \n",
    "# for idx, (_, row) in tqdm(enumerate(se[['actualArrival', 'next_segment_code_1']].iterrows())):\n",
    "#     get_recent_buses(row, idx)\n",
    "    \n",
    "# se['next_segm_last_15_mins_mean'] = last_15_mins_mean\n",
    "# se['next_segm_last_15_mins_count'] = last_15_mins_count \n",
    "# se['next_segm_last_15_30_mins_mean'] = last_15_30_mins_mean \n",
    "# se['next_segm_last_15_30_mins_count'] = last_15_30_mins_count\n",
    "# se['next_segm_last_30_45_mins_mean'] = last_30_45_mins_mean \n",
    "# se['next_segm_last_30_45_mins_count'] = last_30_45_mins_count\n",
    "# se['next_segm_last_45_60_mins_mean'] = last_45_60_mins_mean \n",
    "# se['next_segm_last_45_60_mins_count'] = last_45_60_mins_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 20 #number of partitions to split dataframe\n",
    "num_cores = 5 #number of cores on your machine\n",
    "\n",
    "def parallelize_dataframe(df, func, args):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    \n",
    "    # This line is fidly, we make a list where each item is a tuple of \n",
    "    # a bit of the dataframe and whatever is passed in as args. \n",
    "    # Then starmap unpacks that tuple so each copy of func gets it's \n",
    "    # little bit of the dataframe and the right args to do it's job. \n",
    "    # All this to avoid globals! \n",
    "    all_args = [(split,) + args for split in df_split]\n",
    "    \n",
    "    df = pd.concat(pool.starmap(func, all_args))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def parallelize_dataframe_to_numpy(df, func, args):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    \n",
    "    # This line is fidly, we make a list where each item is a tuple of \n",
    "    # a bit of the dataframe and whatever is passed in as args. \n",
    "    # Then starmap unpacks that tuple so each copy of func gets it's \n",
    "    # little bit of the dataframe and the right args to do it's job. \n",
    "    # All this to avoid globals! \n",
    "    all_args = [(split,) + args for split in df_split]\n",
    "    \n",
    "    matrix = np.concatenate(pool.starmap(func, all_args))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_index = pd.to_datetime(arrival_c.categories)\n",
    "segment_code_index = segment_code_c.categories\n",
    "\n",
    "def calc_segment(se, segment, window_count, window_size = 5):\n",
    "    \n",
    "    last_time_slots_mean = np.empty((se.shape[0], window_count)).astype(float)\n",
    "    last_time_slots_count = np.zeros((se.shape[0], window_count)).astype(int)\n",
    "    \n",
    "    last_time_slots_mean[:,:] = np.nan\n",
    "\n",
    "    def get_recent_buses(row, idx):\n",
    "        \n",
    "        if row[2] == \"\":\n",
    "            return\n",
    "        try:\n",
    "            column_index = segment_code_index.get_loc(row[2])\n",
    "        except KeyError:\n",
    "            return\n",
    "        \n",
    "        for i in range(window_count):\n",
    "            slice_obj = arrival_index.slice_indexer(row[1] - (i+1) * pd.Timedelta(f\"{window_size} min\"), \n",
    "                                                      row[1] - (i * pd.Timedelta(f\"{window_size} min\")) + pd.Timedelta(\"1 sec\"))\n",
    "\n",
    "            journeys = sparse_matrix[slice_obj, column_index].data\n",
    "            \n",
    "            if journeys.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            last_time_slots_mean[idx, i] = journeys.mean()\n",
    "            last_time_slots_count[idx, i] = journeys.shape[0]\n",
    "\n",
    "    \n",
    "    for idx, row in enumerate(se[['actualArrival', segment]].itertuples()):\n",
    "        get_recent_buses(row, idx)\n",
    "            \n",
    "    return np.concatenate((se.index.values.reshape(-1, 1), last_time_slots_mean, last_time_slots_count), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_segments(se, segments, window_count, window_size = 5):\n",
    "    \n",
    "#     arrival_index = pd.to_datetime(arrival_c.categories)\n",
    "#     segment_code_index = segment_code_c.categories\n",
    "    \n",
    "#     last_time_slots_mean = np.empty((se.shape[0], window_count, len(segments))).astype(float)\n",
    "#     last_time_slots_count = np.zeros((se.shape[0], window_count, len(segments))).astype(int)\n",
    "    \n",
    "#     last_time_slots_mean[:,:] = np.nan\n",
    "\n",
    "#     def get_recent_buses(row, idx):\n",
    "            \n",
    "#         column_indicies = []\n",
    "#         column_mask = np.ones(len(segments))\n",
    "        \n",
    "#         for i in range(len(segments)):\n",
    "            \n",
    "#             try:\n",
    "#                 column_indicies.append(segment_code_index.get_loc(row[2]))\n",
    "#             except KeyError:\n",
    "#                 column_mask[i] = 0\n",
    "\n",
    "#         slices = []\n",
    "#         for i in range(window_count):\n",
    "#             slice_obj = arrival_index.slice_indexer(row[1] - (i+1) * pd.Timedelta(f\"{window_size} min\"), \n",
    "#                                                       row[1] - (i * pd.Timedelta(f\"{window_size} min\")) + pd.Timedelta(\"1 sec\"))\n",
    "\n",
    "\n",
    "#             journeys = sparse_matrix[slice_obj,column_indicies].data\n",
    "#             journeys_idx = sparse_matrix[slice_obj,column_indicies].indices\n",
    "#             journeys_idxprt = sparse_matrix[slice_obj,column_indicies].indptr\n",
    "            \n",
    "#             print(\"-------------------------\")\n",
    "#             print(\"##>\" + str(journeys.shape) + \"<##\")\n",
    "#             return\n",
    "            \n",
    "#             if journeys.shape[0] == 0:\n",
    "#                 continue\n",
    "\n",
    "#             last_time_slots_mean[idx, i] = journeys.mean()\n",
    "#             last_time_slots_count[idx, i] = journeys.shape[0]\n",
    "\n",
    "#     columns = ['actualArrival']\n",
    "    \n",
    "#     columns.extend(segments)\n",
    "    \n",
    "#     for idx, row in enumerate(se[columns].itertuples()):\n",
    "#         get_recent_buses(row, idx)\n",
    "        \n",
    "\n",
    "# #     se[f'{segment}_last_{window_count}_{window_size}mins_means'] = last_time_slots_mean.tolist()\n",
    "# #     se[f'{segment}_last_{window_count}_{window_size}mins_counts'] = last_time_slots_count.tolist()\n",
    "    \n",
    "#     return np.concatenate((se.index.values[:,None,None], last_time_slots_mean, last_time_slots_count), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min = se.iloc[0:1000, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = parallelize_dataframe_to_numpy(se[['actualArrival', 'segment_code']], calc_segment, ('segment_code', 12, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With getting the entire column: 16.6 s ± 5.72 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# With indexing into the column: 1.16 s ± 50.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# As above + only making stuff once & no exceptions: 1.09 s ± 68.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_segment_code = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"matrix_segment_code_last_12_15\", matrix_segment_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in ['next_segment_code_1', \n",
    "             'next_segment_code_2', \n",
    "             'next_segment_code_3', \n",
    "             'prev_segment_code_1',\n",
    "             'prev_segment_code_2',\n",
    "             'prev_segment_code_3',\n",
    "             'next_segment_code_4', \n",
    "             'prev_segment_code_4',\n",
    "             'next_segment_code_5', \n",
    "             'prev_segment_code_5',\n",
    "             'next_segment_code_6', \n",
    "             'prev_segment_code_6',\n",
    "            ]:\n",
    "    mtx_last_12_15 = parallelize_dataframe(se[['actualArrival', code]], calc_segment, (code, 12, 15))\n",
    "    np.save(f\"mtx_{code}_last_12_15\", mtx_last_12_15)\n",
    "    print(f\"done: {code}\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mtx_next_segm_code_1 = parallelize_dataframe(se[['actualArrival', 'next_segment_code_1']], calc_segment, ('next_segment_code_1', 12, 15))\n",
    "np.save(\"mtx_next_segm_code_1_last_12_15\", mtx_next_segm_code_1)\n",
    "print(\"*\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_next_segm_code_2 = parallelize_dataframe(se[['actualArrival', 'next_segment_code_2']], calc_segment, ('next_segment_code_2', 12, 15))\n",
    "np.save(\"mtx_next_segm_code_2_last_12_15\", mtx_next_segm_code_2)\n",
    "print(\"*\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_next_segm_code_3 = parallelize_dataframe(se[['actualArrival', 'next_segment_code_3']], calc_segment, ('next_segment_code_3', 12, 15))\n",
    "np.save(\"mtx_next_segm_code_3_last_12_15\", mtx_next_segm_code_3)\n",
    "print(\"*\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_prev_segm_code_1 = parallelize_dataframe(se[['actualArrival', 'prev_segment_code_1']], calc_segment, ('prev_segment_code_1', 12, 15))\n",
    "np.save(\"mtx_prev_segm_code_1_last_12_15\", mtx_prev_segm_code_1)\n",
    "print(\"*\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_prev_segm_code_2 = parallelize_dataframe(se[['actualArrival', 'prev_segment_code_2']], calc_segment, ('prev_segment_code_2', 12, 15))\n",
    "np.save(\"mtx_prev_segm_code_2_last_12_15\", mtx_prev_segm_code_2)\n",
    "print(\"*\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_prev_segm_code_3 = parallelize_dataframe(se[['actualArrival', 'prev_segment_code_3']], calc_segment, ('prev_segment_code_3', 12, 15))\n",
    "np.save(\"mtx_prev_segm_code_3_last_12_15\", mtx_prev_segm_code_3)\n",
    "print(\"*\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>stopCode</th>\n",
       "      <th>aimedArrival</th>\n",
       "      <th>aimedDeparture</th>\n",
       "      <th>actualArrival</th>\n",
       "      <th>actualDeparture</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>workid</th>\n",
       "      <th>patternId</th>\n",
       "      <th>...</th>\n",
       "      <th>last_bus_gap</th>\n",
       "      <th>next_bus_gap</th>\n",
       "      <th>last_this_bus_gap</th>\n",
       "      <th>next_this_bus_gap</th>\n",
       "      <th>dwell_predict_rules_median</th>\n",
       "      <th>dwell_predict_rules_mean</th>\n",
       "      <th>diff_segment_and_mean_by_segment_code</th>\n",
       "      <th>diff_segment_and_mean_by_segment_code_and_hour_and_day</th>\n",
       "      <th>diff_percent_segment_and_mean_by_segment_code</th>\n",
       "      <th>diff_percent_segment_and_mean_by_segment_code_and_hour_and_day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, date, stopCode, aimedArrival, aimedDeparture, actualArrival, actualDeparture, vehicle, workid, patternId, publicName, scheduledStart, prev_stopCode, prev_aimedArrival, prev_aimedDeparture, prev_actualArrival, prev_actualDeparture, timingPoint, prev_timingPoint, segment_code, segment_name, line_distance, midpoint_lat, midpoint_lon, to_centre_dist, direction, direction_degrees, train, test, rain, dwell_duration_dest, dwell_duration_prev, segment_duration, timetable_segment_duration, full_duration, arrival_hour, arrival_day, mean_durations_by_segment_code, mean_durations_by_segment_code_and_hour, mean_durations_by_segment_code_and_hour_and_day, mean_dwell_dest_durations_by_stop_code, mean_dwell_prev_durations_by_stop_code, mean_dwell_dest_by_stop_code_and_hour, mean_dwell_prev_by_stop_code_and_hour, mean_dwell_dest_by_stop_code_and_hour_and_day, mean_dwell_prev_by_stop_code_and_hour_and_day, median_durations_by_segment_code, median_durations_by_segment_code_and_hour, median_durations_by_segment_code_and_hour_and_day, median_dwell_dest_durations_by_stop_code, median_dwell_prev_durations_by_stop_code, median_dwell_dest_by_stop_code_and_hour, median_dwell_prev_by_stop_code_and_hour, median_dwell_dest_by_stop_code_and_hour_and_day, median_dwell_prev_by_stop_code_and_hour_and_day, median_full_durations_by_segment_code, median_full_durations_by_segment_code_and_hour, median_full_durations_by_segment_code_and_hour_and_day, diff_full_segment_and_median_by_segment_code, diff_full_segment_and_median_by_segment_code_and_hour_and_day, diff_percent_full_segment_and_median_by_segment_code, diff_percent_full_segment_and_median_by_segment_code_and_hour_and_day, diff_segment_and_median_by_segment_code, diff_segment_and_median_by_segment_code_and_hour_and_day, diff_percent_segment_and_median_by_segment_code, diff_percent_segment_and_median_by_segment_code_and_hour_and_day, std_diff_percent_segment_median_by_segment_code_and_hour_and_day, prev_segment_code_1, prev_segment_code_2, prev_segment_code_3, prev_segment_code_4, prev_segment_code_5, prev_segment_code_6, prev_segment_code_7, next_segment_code_1, next_segment_code_2, next_segment_code_3, next_segment_code_4, next_segment_code_5, next_segment_code_6, next_segment_code_7, prev_event_index_1, prev_event_index_2, prev_event_index_3, prev_event_index_4, prev_event_index_5, prev_event_index_6, prev_event_index_7, next_event_index_1, next_event_index_2, next_event_index_3, next_event_index_4, next_event_index_5, next_event_index_6, next_event_index_7, clock_direction_degrees, dry, weekend, median_prev_dwell_by_segment_code_and_hour_and_daytype, mean_prev_dwell_by_segment_code_and_hour_and_daytype, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 116 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.loc[se['segment_code'] == \"1290DOB20940_1290DOB20938_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se.reset_index().to_feather(\"with_last_slots2.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min = se.iloc[0:100000, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_all'] = 0\n",
    "\n",
    "mask = np.sum(se_min[['last_15_mins_count', \n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       'prev_segm_last_15_mins_count', \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       'next_segm_last_15_mins_count', \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1) > 0\n",
    "\n",
    "data = se_min.loc[mask, ['last_15_mins_mean', \n",
    "                                                        'last_15_30_mins_mean', \n",
    "                                                        'last_30_45_mins_mean',\n",
    "                                                        'last_45_60_mins_mean',\n",
    "                                                       'prev_segm_last_15_mins_mean', \n",
    "                                                        'prev_segm_last_15_30_mins_mean', \n",
    "                                                        'prev_segm_last_30_45_mins_mean',\n",
    "                                                        'prev_segm_last_45_60_mins_mean',\n",
    "                                                       'next_segm_last_15_mins_mean', \n",
    "                                                        'next_segm_last_15_30_mins_mean', \n",
    "                                                        'next_segm_last_30_45_mins_mean',\n",
    "                                                        'next_segm_last_45_60_mins_mean']]\n",
    "\n",
    "weights = se_min.loc[mask, ['last_15_mins_count', \n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       'prev_segm_last_15_mins_count', \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       'next_segm_last_15_mins_count', \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']]\n",
    "\n",
    "masked_data = np.ma.masked_array(data, np.isnan(data))\n",
    "\n",
    "se_min.loc[mask, 'mean_offsets_enhanced_all'] = np.ma.average(masked_data, \n",
    "                                               axis=1,\n",
    "                                               weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_15plus'] = 0\n",
    "\n",
    "mask = np.sum(se_min[[\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1) > 0\n",
    "\n",
    "data = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_mean', \n",
    "                                                        'last_30_45_mins_mean',\n",
    "                                                        'last_45_60_mins_mean',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_mean', \n",
    "                                                        'prev_segm_last_30_45_mins_mean',\n",
    "                                                        'prev_segm_last_45_60_mins_mean',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_mean', \n",
    "                                                        'next_segm_last_30_45_mins_mean',\n",
    "                                                        'next_segm_last_45_60_mins_mean']]\n",
    "\n",
    "weights = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']]\n",
    "\n",
    "masked_data = np.ma.masked_array(data, np.isnan(data))\n",
    "\n",
    "se_min.loc[mask, 'mean_offsets_enhanced_15plus'] = np.ma.average(masked_data, \n",
    "                                               axis=1,\n",
    "                                               weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_15plus_filtered'] = 0\n",
    "\n",
    "mask = np.sum(se_min[[\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1) > 10\n",
    "\n",
    "data = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_mean', \n",
    "                                                        'last_30_45_mins_mean',\n",
    "                                                        'last_45_60_mins_mean',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_mean', \n",
    "                                                        'prev_segm_last_30_45_mins_mean',\n",
    "                                                        'prev_segm_last_45_60_mins_mean',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_mean', \n",
    "                                                        'next_segm_last_30_45_mins_mean',\n",
    "                                                        'next_segm_last_45_60_mins_mean']]\n",
    "\n",
    "weights = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']]\n",
    "\n",
    "masked_data = np.ma.masked_array(data, np.isnan(data))\n",
    "\n",
    "se_min.loc[mask, 'mean_offsets_enhanced_15plus_filtered'] = np.ma.average(masked_data, \n",
    "                                               axis=1,\n",
    "                                               weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_counts = np.sum(se_min[['last_15_mins_count', \n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       'prev_segm_last_15_mins_count', \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       'next_segm_last_15_mins_count', \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bus_counts, bins=50, range=(0,100));\n",
    "plt.title(\"Number of data points for last hour incl prev & next segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_all'] = se_min['mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min['mean_offsets_enhanced_all']/100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus'] = se_min['mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min['mean_offsets_enhanced_15plus']/100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus_filtered'] = se_min['mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min['mean_offsets_enhanced_15plus_filtered']/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_7'] = se_min['mean_durations_by_segment_code_and_hour_and_day']\n",
    "\n",
    "se_min.loc[se_min['last_30_mins_count'] >= 7, 'mean_durations_by_segment_code_and_hour_and_day_enhanced_7'] = se_min.loc[se_min['last_30_mins_count'] >= 7, 'mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min.loc[se_min['last_30_mins_count'] >= 7, 'last_30_mins_mean']/100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{i} -> {np.count_nonzero(se_min['last_30_mins_count'] >= i)/len(se_min)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(se_min.loc[se_min['last_30_mins_count'] >= 7, 'last_30_mins_mean'], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array_median_chd = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_median_chd[:] = np.nan\n",
    "\n",
    "predict_array_mean_chd = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_chd[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh7 = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh7[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh_all = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh_all[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh_15plus = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh_15plus[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh_15plus_filtered = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh_15plus_filtered[:] = np.nan\n",
    "\n",
    "predict_array_median_rules = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_median_rules[:] = np.nan\n",
    "\n",
    "predict_array_mean_rules = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_rules[:] = np.nan\n",
    "\n",
    "actual_array = np.empty((se_min.shape[0],140)).astype(float)\n",
    "actual_array[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest the index so that it matches the index into the numpy array\n",
    "\n",
    "se_min.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = se_min.groupby(['date','workid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, run in tqdm(runs):\n",
    "    run = run.sort_values(\"actualArrival\")\n",
    "    \n",
    "    run_length = len(run)\n",
    "    \n",
    "    for i in range(min([run_length, 70])):\n",
    "        \n",
    "        predict_array_median_chd[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['median_durations_by_segment_code_and_hour_and_day', 'median_dwell_prev_by_stop_code_and_hour_and_day']]\n",
    "        \n",
    "        predict_array_mean_chd[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day', 'mean_dwell_prev_by_stop_code_and_hour_and_day']]\n",
    "        \n",
    "#         predict_array_mean_enh[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced', 'median_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "#         predict_array_mean_enh7[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_7', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "        predict_array_mean_enh_all[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_all', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "        predict_array_mean_enh_15plus[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "        predict_array_mean_enh_15plus_filtered[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus_filtered', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "#         predict_array_median_rules[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code', 'dwell_predict_rules_median']]\n",
    "        \n",
    "#         predict_array_mean_rules[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code', 'dwell_predict_rules_mean']]\n",
    "    \n",
    "        actual_array[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['segment_duration', 'dwell_duration_prev']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = []\n",
    "\n",
    "for i in range(140):\n",
    "    nans.append(predict_array_median_chd.shape[0] - np.count_nonzero(np.isnan(predict_array_median_chd[:,i])))\n",
    "    \n",
    "plt.plot(nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_in_x_percent(predict, actual, threshold):\n",
    "    \n",
    "    if np.count_nonzero(~np.isnan(actual)) == 0:\n",
    "        return 0\n",
    "    \n",
    "    threshold = threshold/100\n",
    "    \n",
    "    mask = (~np.isnan(predict) & ~np.isnan(actual))\n",
    "    \n",
    "    pass_count = np.count_nonzero((predict[mask] < actual[mask] * (1 + threshold)) & (predict[mask] > actual[mask] * (1-threshold)))\n",
    "    \n",
    "    over_count = np.count_nonzero(predict[mask] > actual[mask] * (1+threshold))\n",
    "    \n",
    "    under_count = np.count_nonzero(predict[mask] < actual[mask] * (1-threshold))\n",
    "    \n",
    "    pass_percent = pass_count/np.count_nonzero(mask) * 100\n",
    "    \n",
    "    if over_count + under_count == 0:\n",
    "        drift = 0.5\n",
    "    else:\n",
    "        drift = over_count / (over_count + under_count)\n",
    "    \n",
    "    return pass_percent, drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_accuracy_matrix_minutes(predict, actual, max_threshold = 50):\n",
    "\n",
    "    actual_ints = np.array(actual/60).astype(int)\n",
    "    \n",
    "    rows = int(max_threshold/10)\n",
    "    \n",
    "    max_a = np.nanmax(actual)/60\n",
    "\n",
    "    accuracies_table = np.empty((rows, int(max_a)))\n",
    "    drift_table = np.empty((rows, int(max_a)))\n",
    "    frequency = np.empty(int(max_a))\n",
    "\n",
    "    for i in range(int(max_a)):\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        mask = (actual_ints == i)\n",
    "        \n",
    "        frequency[i] = np.count_nonzero(mask)\n",
    "        \n",
    "        for j in range(1, rows+1):\n",
    "            accuracy, drift = percent_in_x_percent(predict[mask], actual[mask], j * 10)\n",
    "            accuracies_table[j-1,i] = accuracy\n",
    "            drift_table[j-1, i] = drift\n",
    "\n",
    "    return accuracies_table, frequency, drift_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy_minutes(predict, actual, title):\n",
    "    results, frequency, drift = make_accuracy_matrix_minutes(predict, actual)\n",
    "    \n",
    "    for i in range(results.shape[0]):\n",
    "        plt.plot(results[i,:], label=f\"{(i+1)*10}%\")\n",
    "        \n",
    "        \n",
    "    plt.xlabel(\"minutes ahead\")\n",
    "    plt.ylabel(\"percentage within threshold\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlim(0,20)\n",
    "    plt.ylim(0,100)\n",
    "    plt.gca().yaxis.grid(True, linewidth=\"0.2\")\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(drift[0,:], label=\"fraction over\", linestyle=\":\")\n",
    "    ax2.set_ylim(0,1)\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array_median_chd_cum = np.cumsum(predict_array_median_chd, axis=1)\n",
    "predict_array_mean_chd_cum = np.cumsum(predict_array_mean_chd, axis=1)\n",
    "# predict_array_mean_enh_cum = np.cumsum(predict_array_mean_enh, axis=1)\n",
    "# predict_array_mean_enh7_cum = np.cumsum(predict_array_mean_enh7, axis=1)\n",
    "predict_array_mean_enh_all_cum = np.cumsum(predict_array_mean_enh_all, axis=1)\n",
    "predict_array_mean_enh_15plus_cum = np.cumsum(predict_array_mean_enh_15plus, axis=1)\n",
    "predict_array_mean_enh_15plus_filtered_cum = np.cumsum(predict_array_mean_enh_15plus_filtered, axis=1)\n",
    "# predict_array_median_rules_cum = np.cumsum(predict_array_median_rules, axis=1)\n",
    "# predict_array_mean_rules_cum = np.cumsum(predict_array_mean_rules, axis=1)\n",
    "\n",
    "\n",
    "actual_array_cum = np.cumsum(actual_array, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_journey_median_chd_cum = np.cumsum(predict_array_median_chd[:, ::2], axis=1)\n",
    "predict_journey_mean_chd_cum = np.cumsum(predict_array_mean_chd[:, ::2], axis=1)\n",
    "# predict_journey_mean_enh_cum = np.cumsum(predict_array_mean_enh[:, ::2], axis=1)\n",
    "# predict_journey_mean_enh7_cum = np.cumsum(predict_array_mean_enh7[:, ::2], axis=1)\n",
    "predict_journey_mean_enh_all_cum = np.cumsum(predict_array_mean_enh_all[:, ::2], axis=1)\n",
    "predict_journey_mean_enh_15plus_cum = np.cumsum(predict_array_mean_enh_15plus[:, ::2], axis=1)\n",
    "predict_journey_mean_enh_15plus_filtered_cum = np.cumsum(predict_array_mean_enh_15plus_filtered[:, ::2], axis=1)\n",
    "actual_journey_cum = np.clip(np.cumsum(actual_array[:, ::2], axis=1), 0, 2*60*60)\n",
    "\n",
    "predict_dwell_median_chd_cum = np.cumsum(predict_array_median_chd[:, 1::2], axis=1)\n",
    "predict_dwell_mean_chd_cum = np.cumsum(predict_array_mean_chd[:, 1::2], axis=1)\n",
    "actual_dwell_cum = np.clip(np.cumsum(actual_array[:, 1::2], axis=1), 0, 2*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_dwell_median_rules_cum = np.cumsum(predict_array_median_rules[:, 1::2], axis=1)\n",
    "# predict_dwell_mean_rules_cum = np.cumsum(predict_array_mean_rules[:, 1::2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_array_cum = np.clip(actual_array_cum, 0, 2*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_mean_chd_cum, actual_array_cum, \"full journey accuracies means (chd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_median_chd_cum, actual_array_cum, \"full journey accuracies median (chd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_mean_enh_all_cum, actual_array_cum, \"full journey accuracies mean enhanced all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_mean_enh7_cum, actual_array_cum, \"full journey accuracies mean enhanced 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_journey_mean_chd_cum, actual_journey_cum, \"just journey accuracies means (chd)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_journey_median_chd_cum, actual_journey_cum, \"just journey accuracies medians (chd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_journey_mean_enh7_cum, actual_journey_cum, \"just journey accuracies means (enh7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, _, _= make_accuracy_matrix_minutes(predict_journey_mean_chd_cum, actual_journey_cum, 10)\n",
    "results_enh7, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh7_cum, actual_journey_cum, 10)\n",
    "results_enh_all, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh_all_cum, actual_journey_cum, 10)\n",
    "results_enh_15plus, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh_15plus_cum, actual_journey_cum, 10)\n",
    "results_enh_15plus_filtered, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh_15plus_filtered_cum, actual_journey_cum, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results[0,:], label=\"mean\")\n",
    "# plt.plot(results_enh7[0,:], label=\"mean_enh7\")\n",
    "plt.plot(results_enh_all[0,:], label=\"mean_enh_all\")\n",
    "plt.plot(results_enh_15plus[0,:], label=\"mean_enh_15plus\")\n",
    "plt.plot(results_enh_15plus_filtered[0,:], label=\"mean_enh_15plus_filtered\")\n",
    "plt.legend()\n",
    "plt.xlim(0,20)\n",
    "plt.ylim(20,60)\n",
    "plt.title(\"Percentage of predictions within 10%\")\n",
    "plt.xlabel(\"minutes ahead\")\n",
    "plt.ylabel(\"percent within 10%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "busses",
   "language": "python",
   "name": "busses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
