{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import feather\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from tqdm import tqdm_pandas\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = feather.read_dataframe(\"../data_files/B/once/75days/stop_events_with_geo_train_test_averages_prev_next_dwell.feather\")\n",
    "# se = feather.read_dataframe(\"../data_files/B/once/75days/se_prev_next.feather\")\n",
    "se = se.set_index(se.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # And now for just segments:\n",
    "se[\"diff_segment_and_mean_by_segment_code\"] = (\n",
    "    se[\"segment_duration\"]\n",
    "    - se[\"mean_durations_by_segment_code\"]\n",
    ")\n",
    "se[\"diff_segment_and_mean_by_segment_code_and_hour_and_day\"] = (\n",
    "    se[\"segment_duration\"]\n",
    "    - se[\"mean_durations_by_segment_code_and_hour_and_day\"]\n",
    ")\n",
    "\n",
    "se[\"diff_percent_segment_and_mean_by_segment_code\"] = (\n",
    "    se[\"diff_segment_and_mean_by_segment_code\"]\n",
    "    * 100\n",
    "    / se[\"mean_durations_by_segment_code\"]\n",
    ")\n",
    "\n",
    "se[\"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\"] = (\n",
    "    se[\"diff_segment_and_mean_by_segment_code_and_hour_and_day\"]\n",
    "    * 100\n",
    "    / se[\"mean_durations_by_segment_code_and_hour_and_day\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_columns_containing(se, to_include):\n",
    "\n",
    "    min_cols = [c for c in se.columns if any(x in c for x in to_include)]\n",
    "\n",
    "    se_min = se[min_cols]\n",
    "\n",
    "    return se_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_columns_containing(se, to_remove):\n",
    "\n",
    "    min_cols = [c for c in se.columns if not any(x in c for x in to_remove)]\n",
    "\n",
    "    se_min = se[min_cols]\n",
    "\n",
    "    return se_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = exclude_columns_containing(se, [\"prev_segment_code_\", \"next_segment_code_\", \"prev_event_index_\", \"next_event_index_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_prev_next = include_columns_containing(se_prev_next, ['workid', 'date', 'segment_code', 'prev_segment_code_', 'next_segment_code_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = se.merge(se_prev_next, on=['workid', 'date', 'segment_code'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate this from scratch as we need both test and train data.\n",
    "# ts_5 = se.pivot_table(\n",
    "#     index=\"actualArrival\",\n",
    "#     columns=\"segment_code\",\n",
    "#     values=\"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\",\n",
    "#     aggfunc=np.median,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://stackoverflow.com/questions/31661604/efficiently-create-sparse-pivot-tables-in-pandas\n",
    "\n",
    "arrival_c = CategoricalDtype(sorted(se.actualArrival.unique()), ordered=True)\n",
    "segment_code_c = CategoricalDtype(sorted(se.segment_code.unique()), ordered=True)\n",
    "\n",
    "row = se.actualArrival.astype(arrival_c).cat.codes\n",
    "col = se.segment_code.astype(segment_code_c).cat.codes\n",
    "sparse_matrix = csr_matrix((se[\"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\"], (row, col)), \\\n",
    "                           shape=(arrival_c.categories.size, segment_code_c.categories.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts.loc['2018-12-16 14:00:00':'2018-12-16 14:08:01', '1200BOB20146_1200DOY38562_0'].dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 20 #number of partitions to split dataframe\n",
    "num_cores = 5 #number of cores on your machine\n",
    "\n",
    "def parallelize_dataframe_to_numpy(df, func, args):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    \n",
    "    # This line is fidly, we make a list where each item is a tuple of \n",
    "    # a bit of the dataframe and whatever is passed in as args. \n",
    "    # Then starmap unpacks that tuple so each copy of func gets it's \n",
    "    # little bit of the dataframe and the right args to do it's job. \n",
    "    # All this to avoid globals! \n",
    "    all_args = [(split,) + args for split in df_split]\n",
    "    \n",
    "    matrix = np.concatenate(pool.starmap(func, all_args))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_index = pd.to_datetime(arrival_c.categories)\n",
    "segment_code_index = segment_code_c.categories\n",
    "\n",
    "def calc_segment(se, segment, window_count, window_size = 5):\n",
    "    \n",
    "    last_time_slots_mean = np.empty((se.shape[0], window_count)).astype(float)\n",
    "    last_time_slots_count = np.zeros((se.shape[0], window_count)).astype(int)\n",
    "    \n",
    "    last_time_slots_mean[:,:] = np.nan\n",
    "\n",
    "    def get_recent_buses(row, idx):\n",
    "        \n",
    "        if row[2] == \"\":\n",
    "            return\n",
    "        try:\n",
    "            column_index = segment_code_index.get_loc(row[2])\n",
    "        except KeyError:\n",
    "            return\n",
    "        \n",
    "        for i in range(window_count):\n",
    "            slice_obj = arrival_index.slice_indexer(row[1] - (i+1) * pd.Timedelta(f\"{window_size} min\"), \n",
    "                                                      row[1] - (i * pd.Timedelta(f\"{window_size} min\")) + pd.Timedelta(\"1 sec\"))\n",
    "\n",
    "            journeys = sparse_matrix[slice_obj, column_index].data\n",
    "            \n",
    "            if journeys.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            last_time_slots_mean[idx, i] = journeys.mean()\n",
    "            last_time_slots_count[idx, i] = journeys.shape[0]\n",
    "\n",
    "    \n",
    "    for idx, row in enumerate(se[['actualArrival', segment]].itertuples()):\n",
    "        get_recent_buses(row, idx)\n",
    "            \n",
    "    return np.concatenate((se.index.values.reshape(-1, 1), last_time_slots_mean, last_time_slots_count), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = parallelize_dataframe_to_numpy(se[['actualArrival', 'segment_code']], calc_segment, ('segment_code', 12, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With getting the entire column: 16.6 s ± 5.72 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# With indexing into the column: 1.16 s ± 50.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# As above + only making stuff once & no exceptions: 1.09 s ± 68.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_segment_code = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"matrix_segment_code_last_12_15\", matrix_segment_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: segment_code\n",
      "done: next_segment_code_1\n",
      "done: next_segment_code_2\n",
      "done: next_segment_code_3\n",
      "done: prev_segment_code_1\n",
      "done: prev_segment_code_2\n"
     ]
    }
   ],
   "source": [
    "for code in ['segment_code',\n",
    "             'next_segment_code_1', \n",
    "             'next_segment_code_2', \n",
    "             'next_segment_code_3', \n",
    "             'prev_segment_code_1',\n",
    "             'prev_segment_code_2',\n",
    "             'prev_segment_code_3',\n",
    "             'next_segment_code_4', \n",
    "             'prev_segment_code_4',\n",
    "             'next_segment_code_5', \n",
    "             'prev_segment_code_5',\n",
    "             'next_segment_code_6', \n",
    "             'prev_segment_code_6',\n",
    "             'next_segment_code_7', \n",
    "             'prev_segment_code_7',\n",
    "             'next_segment_code_8', \n",
    "             'prev_segment_code_8',\n",
    "             'next_segment_code_9', \n",
    "             'prev_segment_code_9',\n",
    "             'next_segment_code_10', \n",
    "             'prev_segment_code_10',\n",
    "             'next_segment_code_11', \n",
    "             'prev_segment_code_11',\n",
    "            ]:\n",
    "    mtx_last_25_10 = parallelize_dataframe_to_numpy(se[['actualArrival', code]], calc_segment, (code, 25, 10))\n",
    "    np.save(f\"mtx_{code}_last_25_10\", mtx_last_25_10)\n",
    "    print(f\"done: {code}\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_all'] = 0\n",
    "\n",
    "mask = np.sum(se_min[['last_15_mins_count', \n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       'prev_segm_last_15_mins_count', \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       'next_segm_last_15_mins_count', \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1) > 0\n",
    "\n",
    "data = se_min.loc[mask, ['last_15_mins_mean', \n",
    "                                                        'last_15_30_mins_mean', \n",
    "                                                        'last_30_45_mins_mean',\n",
    "                                                        'last_45_60_mins_mean',\n",
    "                                                       'prev_segm_last_15_mins_mean', \n",
    "                                                        'prev_segm_last_15_30_mins_mean', \n",
    "                                                        'prev_segm_last_30_45_mins_mean',\n",
    "                                                        'prev_segm_last_45_60_mins_mean',\n",
    "                                                       'next_segm_last_15_mins_mean', \n",
    "                                                        'next_segm_last_15_30_mins_mean', \n",
    "                                                        'next_segm_last_30_45_mins_mean',\n",
    "                                                        'next_segm_last_45_60_mins_mean']]\n",
    "\n",
    "weights = se_min.loc[mask, ['last_15_mins_count', \n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       'prev_segm_last_15_mins_count', \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       'next_segm_last_15_mins_count', \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']]\n",
    "\n",
    "masked_data = np.ma.masked_array(data, np.isnan(data))\n",
    "\n",
    "se_min.loc[mask, 'mean_offsets_enhanced_all'] = np.ma.average(masked_data, \n",
    "                                               axis=1,\n",
    "                                               weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_15plus'] = 0\n",
    "\n",
    "mask = np.sum(se_min[[\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1) > 0\n",
    "\n",
    "data = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_mean', \n",
    "                                                        'last_30_45_mins_mean',\n",
    "                                                        'last_45_60_mins_mean',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_mean', \n",
    "                                                        'prev_segm_last_30_45_mins_mean',\n",
    "                                                        'prev_segm_last_45_60_mins_mean',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_mean', \n",
    "                                                        'next_segm_last_30_45_mins_mean',\n",
    "                                                        'next_segm_last_45_60_mins_mean']]\n",
    "\n",
    "weights = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']]\n",
    "\n",
    "masked_data = np.ma.masked_array(data, np.isnan(data))\n",
    "\n",
    "se_min.loc[mask, 'mean_offsets_enhanced_15plus'] = np.ma.average(masked_data, \n",
    "                                               axis=1,\n",
    "                                               weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_15plus_filtered'] = 0\n",
    "\n",
    "mask = np.sum(se_min[[\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1) > 10\n",
    "\n",
    "data = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_mean', \n",
    "                                                        'last_30_45_mins_mean',\n",
    "                                                        'last_45_60_mins_mean',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_mean', \n",
    "                                                        'prev_segm_last_30_45_mins_mean',\n",
    "                                                        'prev_segm_last_45_60_mins_mean',\n",
    "                                                       \n",
    "                                                        'next_segm_last_15_30_mins_mean', \n",
    "                                                        'next_segm_last_30_45_mins_mean',\n",
    "                                                        'next_segm_last_45_60_mins_mean']]\n",
    "\n",
    "weights = se_min.loc[mask, [\n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                      \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']]\n",
    "\n",
    "masked_data = np.ma.masked_array(data, np.isnan(data))\n",
    "\n",
    "se_min.loc[mask, 'mean_offsets_enhanced_15plus_filtered'] = np.ma.average(masked_data, \n",
    "                                               axis=1,\n",
    "                                               weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_counts = np.sum(se_min[['last_15_mins_count', \n",
    "                                                        'last_15_30_mins_count', \n",
    "                                                        'last_30_45_mins_count',\n",
    "                                                        'last_45_60_mins_count',\n",
    "                                                       'prev_segm_last_15_mins_count', \n",
    "                                                        'prev_segm_last_15_30_mins_count', \n",
    "                                                        'prev_segm_last_30_45_mins_count',\n",
    "                                                        'prev_segm_last_45_60_mins_count',\n",
    "                                                       'next_segm_last_15_mins_count', \n",
    "                                                        'next_segm_last_15_30_mins_count', \n",
    "                                                        'next_segm_last_30_45_mins_count',\n",
    "                                                        'next_segm_last_45_60_mins_count']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bus_counts, bins=50, range=(0,100));\n",
    "plt.title(\"Number of data points for last hour incl prev & next segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_offsets_enhanced_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_all'] = se_min['mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min['mean_offsets_enhanced_all']/100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus'] = se_min['mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min['mean_offsets_enhanced_15plus']/100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus_filtered'] = se_min['mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min['mean_offsets_enhanced_15plus_filtered']/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_min['mean_durations_by_segment_code_and_hour_and_day_enhanced_7'] = se_min['mean_durations_by_segment_code_and_hour_and_day']\n",
    "\n",
    "se_min.loc[se_min['last_30_mins_count'] >= 7, 'mean_durations_by_segment_code_and_hour_and_day_enhanced_7'] = se_min.loc[se_min['last_30_mins_count'] >= 7, 'mean_durations_by_segment_code_and_hour_and_day'] * (1 + (se_min.loc[se_min['last_30_mins_count'] >= 7, 'last_30_mins_mean']/100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{i} -> {np.count_nonzero(se_min['last_30_mins_count'] >= i)/len(se_min)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(se_min.loc[se_min['last_30_mins_count'] >= 7, 'last_30_mins_mean'], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array_median_chd = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_median_chd[:] = np.nan\n",
    "\n",
    "predict_array_mean_chd = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_chd[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh7 = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh7[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh_all = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh_all[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh_15plus = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh_15plus[:] = np.nan\n",
    "\n",
    "predict_array_mean_enh_15plus_filtered = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_enh_15plus_filtered[:] = np.nan\n",
    "\n",
    "predict_array_median_rules = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_median_rules[:] = np.nan\n",
    "\n",
    "predict_array_mean_rules = np.empty((se_min.shape[0],140)).astype(float)\n",
    "predict_array_mean_rules[:] = np.nan\n",
    "\n",
    "actual_array = np.empty((se_min.shape[0],140)).astype(float)\n",
    "actual_array[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest the index so that it matches the index into the numpy array\n",
    "\n",
    "se_min.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = se_min.groupby(['date','workid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, run in tqdm(runs):\n",
    "    run = run.sort_values(\"actualArrival\")\n",
    "    \n",
    "    run_length = len(run)\n",
    "    \n",
    "    for i in range(min([run_length, 70])):\n",
    "        \n",
    "        predict_array_median_chd[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['median_durations_by_segment_code_and_hour_and_day', 'median_dwell_prev_by_stop_code_and_hour_and_day']]\n",
    "        \n",
    "        predict_array_mean_chd[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day', 'mean_dwell_prev_by_stop_code_and_hour_and_day']]\n",
    "        \n",
    "#         predict_array_mean_enh[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced', 'median_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "#         predict_array_mean_enh7[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_7', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "        predict_array_mean_enh_all[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_all', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "        predict_array_mean_enh_15plus[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "        predict_array_mean_enh_15plus_filtered[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code_and_hour_and_day_enhanced_15plus_filtered', 'mean_dwell_prev_durations_by_stop_code']]\n",
    "        \n",
    "#         predict_array_median_rules[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code', 'dwell_predict_rules_median']]\n",
    "        \n",
    "#         predict_array_mean_rules[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['mean_durations_by_segment_code', 'dwell_predict_rules_mean']]\n",
    "    \n",
    "        actual_array[run.iloc[i:].index,i*2:i*2+2] = run.iloc[:run_length-i][['segment_duration', 'dwell_duration_prev']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = []\n",
    "\n",
    "for i in range(140):\n",
    "    nans.append(predict_array_median_chd.shape[0] - np.count_nonzero(np.isnan(predict_array_median_chd[:,i])))\n",
    "    \n",
    "plt.plot(nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_in_x_percent(predict, actual, threshold):\n",
    "    \n",
    "    if np.count_nonzero(~np.isnan(actual)) == 0:\n",
    "        return 0\n",
    "    \n",
    "    threshold = threshold/100\n",
    "    \n",
    "    mask = (~np.isnan(predict) & ~np.isnan(actual))\n",
    "    \n",
    "    pass_count = np.count_nonzero((predict[mask] < actual[mask] * (1 + threshold)) & (predict[mask] > actual[mask] * (1-threshold)))\n",
    "    \n",
    "    over_count = np.count_nonzero(predict[mask] > actual[mask] * (1+threshold))\n",
    "    \n",
    "    under_count = np.count_nonzero(predict[mask] < actual[mask] * (1-threshold))\n",
    "    \n",
    "    pass_percent = pass_count/np.count_nonzero(mask) * 100\n",
    "    \n",
    "    if over_count + under_count == 0:\n",
    "        drift = 0.5\n",
    "    else:\n",
    "        drift = over_count / (over_count + under_count)\n",
    "    \n",
    "    return pass_percent, drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_accuracy_matrix_minutes(predict, actual, max_threshold = 50):\n",
    "\n",
    "    actual_ints = np.array(actual/60).astype(int)\n",
    "    \n",
    "    rows = int(max_threshold/10)\n",
    "    \n",
    "    max_a = np.nanmax(actual)/60\n",
    "\n",
    "    accuracies_table = np.empty((rows, int(max_a)))\n",
    "    drift_table = np.empty((rows, int(max_a)))\n",
    "    frequency = np.empty(int(max_a))\n",
    "\n",
    "    for i in range(int(max_a)):\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        mask = (actual_ints == i)\n",
    "        \n",
    "        frequency[i] = np.count_nonzero(mask)\n",
    "        \n",
    "        for j in range(1, rows+1):\n",
    "            accuracy, drift = percent_in_x_percent(predict[mask], actual[mask], j * 10)\n",
    "            accuracies_table[j-1,i] = accuracy\n",
    "            drift_table[j-1, i] = drift\n",
    "\n",
    "    return accuracies_table, frequency, drift_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy_minutes(predict, actual, title):\n",
    "    results, frequency, drift = make_accuracy_matrix_minutes(predict, actual)\n",
    "    \n",
    "    for i in range(results.shape[0]):\n",
    "        plt.plot(results[i,:], label=f\"{(i+1)*10}%\")\n",
    "        \n",
    "        \n",
    "    plt.xlabel(\"minutes ahead\")\n",
    "    plt.ylabel(\"percentage within threshold\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlim(0,20)\n",
    "    plt.ylim(0,100)\n",
    "    plt.gca().yaxis.grid(True, linewidth=\"0.2\")\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(drift[0,:], label=\"fraction over\", linestyle=\":\")\n",
    "    ax2.set_ylim(0,1)\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array_median_chd_cum = np.cumsum(predict_array_median_chd, axis=1)\n",
    "predict_array_mean_chd_cum = np.cumsum(predict_array_mean_chd, axis=1)\n",
    "# predict_array_mean_enh_cum = np.cumsum(predict_array_mean_enh, axis=1)\n",
    "# predict_array_mean_enh7_cum = np.cumsum(predict_array_mean_enh7, axis=1)\n",
    "predict_array_mean_enh_all_cum = np.cumsum(predict_array_mean_enh_all, axis=1)\n",
    "predict_array_mean_enh_15plus_cum = np.cumsum(predict_array_mean_enh_15plus, axis=1)\n",
    "predict_array_mean_enh_15plus_filtered_cum = np.cumsum(predict_array_mean_enh_15plus_filtered, axis=1)\n",
    "# predict_array_median_rules_cum = np.cumsum(predict_array_median_rules, axis=1)\n",
    "# predict_array_mean_rules_cum = np.cumsum(predict_array_mean_rules, axis=1)\n",
    "\n",
    "\n",
    "actual_array_cum = np.cumsum(actual_array, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_journey_median_chd_cum = np.cumsum(predict_array_median_chd[:, ::2], axis=1)\n",
    "predict_journey_mean_chd_cum = np.cumsum(predict_array_mean_chd[:, ::2], axis=1)\n",
    "# predict_journey_mean_enh_cum = np.cumsum(predict_array_mean_enh[:, ::2], axis=1)\n",
    "# predict_journey_mean_enh7_cum = np.cumsum(predict_array_mean_enh7[:, ::2], axis=1)\n",
    "predict_journey_mean_enh_all_cum = np.cumsum(predict_array_mean_enh_all[:, ::2], axis=1)\n",
    "predict_journey_mean_enh_15plus_cum = np.cumsum(predict_array_mean_enh_15plus[:, ::2], axis=1)\n",
    "predict_journey_mean_enh_15plus_filtered_cum = np.cumsum(predict_array_mean_enh_15plus_filtered[:, ::2], axis=1)\n",
    "actual_journey_cum = np.clip(np.cumsum(actual_array[:, ::2], axis=1), 0, 2*60*60)\n",
    "\n",
    "predict_dwell_median_chd_cum = np.cumsum(predict_array_median_chd[:, 1::2], axis=1)\n",
    "predict_dwell_mean_chd_cum = np.cumsum(predict_array_mean_chd[:, 1::2], axis=1)\n",
    "actual_dwell_cum = np.clip(np.cumsum(actual_array[:, 1::2], axis=1), 0, 2*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_dwell_median_rules_cum = np.cumsum(predict_array_median_rules[:, 1::2], axis=1)\n",
    "# predict_dwell_mean_rules_cum = np.cumsum(predict_array_mean_rules[:, 1::2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_array_cum = np.clip(actual_array_cum, 0, 2*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_mean_chd_cum, actual_array_cum, \"full journey accuracies means (chd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_median_chd_cum, actual_array_cum, \"full journey accuracies median (chd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_mean_enh_all_cum, actual_array_cum, \"full journey accuracies mean enhanced all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_array_mean_enh7_cum, actual_array_cum, \"full journey accuracies mean enhanced 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_journey_mean_chd_cum, actual_journey_cum, \"just journey accuracies means (chd)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_journey_median_chd_cum, actual_journey_cum, \"just journey accuracies medians (chd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracy_minutes(predict_journey_mean_enh7_cum, actual_journey_cum, \"just journey accuracies means (enh7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, _, _= make_accuracy_matrix_minutes(predict_journey_mean_chd_cum, actual_journey_cum, 10)\n",
    "results_enh7, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh7_cum, actual_journey_cum, 10)\n",
    "results_enh_all, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh_all_cum, actual_journey_cum, 10)\n",
    "results_enh_15plus, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh_15plus_cum, actual_journey_cum, 10)\n",
    "results_enh_15plus_filtered, _, _ = make_accuracy_matrix_minutes(predict_journey_mean_enh_15plus_filtered_cum, actual_journey_cum, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results[0,:], label=\"mean\")\n",
    "# plt.plot(results_enh7[0,:], label=\"mean_enh7\")\n",
    "plt.plot(results_enh_all[0,:], label=\"mean_enh_all\")\n",
    "plt.plot(results_enh_15plus[0,:], label=\"mean_enh_15plus\")\n",
    "plt.plot(results_enh_15plus_filtered[0,:], label=\"mean_enh_15plus_filtered\")\n",
    "plt.legend()\n",
    "plt.xlim(0,20)\n",
    "plt.ylim(20,60)\n",
    "plt.title(\"Percentage of predictions within 10%\")\n",
    "plt.xlabel(\"minutes ahead\")\n",
    "plt.ylabel(\"percent within 10%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "busses",
   "language": "python",
   "name": "busses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
