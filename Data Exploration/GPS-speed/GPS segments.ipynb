{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import radians, degrees, cos, sin, asin, sqrt\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm as tqdm_t\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import feather\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers, Input, Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, LSTM, Dropout\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import utm\n",
    "\n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction = 4/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = feather.read_dataframe(\"../../data_files/B/once/75days/stop_events_with_geo_train_test_averages_prev_next_dwell.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se.columns[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = feather.read_dataframe(\"../../data_files/B/once/75days/stop_events_with_geo_train_test_averages_prev_next_dwell.feather\",\n",
    "                            columns=[\"index\",\n",
    "                                     \"segment_duration\",\n",
    "                                     \"mean_durations_by_segment_code_and_hour_and_day\",\n",
    "                                     \"diff_segment_and_mean_by_segment_code\",\n",
    "                                     \"diff_segment_and_mean_by_segment_code_and_hour_and_day\",\n",
    "                                     'line_distance',\n",
    "                                     'to_centre_dist',\n",
    "                                     'direction_degrees',\n",
    "                                     'rain',\n",
    "                                     'median_durations_by_segment_code_and_hour_and_day',\n",
    "                                     'arrival_hour','arrival_day',\n",
    "                                     \"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\",\n",
    "                                     'date','workid',\n",
    "                                     'actualArrival',\n",
    "                                     'publicName',\n",
    "                                     'segment_name',\n",
    "                                     'prev_segment_code_1',\n",
    "                                     'next_segment_code_1'\n",
    "                                    ])\n",
    "se = se.set_index(se.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = pd.read_csv(\"../../Bournemouth GTFS/routes.csv\")\n",
    "shapes = pd.read_csv(\"../../Bournemouth GTFS/shapes.csv\")\n",
    "stops = pd.read_csv(\"../../Bournemouth GTFS/stops.txt\").set_index('stop_id')\n",
    "trips = pd.read_csv(\"../../Bournemouth GTFS/trips.txt\")\n",
    "performed_work = pd.read_csv(\"../../Trapeze_Data/first dump/PerformedWork.csv\", parse_dates=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_gps_data_hour = np.load(\"se_gps_features_20min_80min_5.npy\")\n",
    "se_gps_data_20mins = np.load(\"se_gps_features_20min_40min_5.npy\", )\n",
    "se_gps_data_hour_prev_next = np.load(\"se_gps_features_20min_80min_prev_next_5.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se['speed_mph'] = se['line_distance']*1000/se['segment_duration'] * 2.237\n",
    "\n",
    "# se['speed_mph_baseline'] = se['line_distance']*1000/se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237\n",
    "\n",
    "se[\"diff_segment_and_mean_by_segment_code_and_hour_and_day\"] = (\n",
    "    se[\"segment_duration\"]\n",
    "    - se[\"mean_durations_by_segment_code_and_hour_and_day\"]\n",
    ")\n",
    "\n",
    "se[\"diff_percent_segment_and_mean_by_segment_code_and_hour_and_day\"] = (\n",
    "    se[\"diff_segment_and_mean_by_segment_code_and_hour_and_day\"]\n",
    "    * 100\n",
    "    / se[\"mean_durations_by_segment_code_and_hour_and_day\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se['prev_segment_name'] = se['prev_segment_code_1'].str[:-2]\n",
    "se['next_segment_name'] = se['next_segment_code_1'].str[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_to_shapes_dict = {}\n",
    "\n",
    "for route_id, route_group in trips.groupby('route_id'):\n",
    "    matching_shapes = route_group.groupby('shape_id').first().index.values\n",
    "    \n",
    "    trips_to_shapes_dict[route_id] = matching_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_dict = {}\n",
    "\n",
    "for shape_name, shape in tqdm(shapes.groupby('shape_id')):\n",
    "    \n",
    "    coords = np.empty((len(shape)+1, 2))\n",
    "    \n",
    "    for i, row in enumerate(shape[['latitude','longitude']].itertuples()):\n",
    "\n",
    "        coords[i, :] = utm.from_latlon(row[1], row[2])[:2]\n",
    "                        \n",
    "    shapes_dict[shape_name] = LineString(coords)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_cache = {}\n",
    "\n",
    "def find_length_by_name_route(segment_name, route_id, line_dist):\n",
    "    \n",
    "    line_dist = line_dist * 1000\n",
    "    \n",
    "    if segment_name in segment_cache:\n",
    "        return segment_cache[segment_name]\n",
    "    \n",
    "    if type(route_id) == float:\n",
    "        if np.isnan(route_id):\n",
    "            \n",
    "            segment_cache[segment_name] = line_dist\n",
    "            \n",
    "            return line_dist\n",
    "    \n",
    "    best_dist = np.inf\n",
    "    lengths = []\n",
    "    distances = []\n",
    "    \n",
    "    stop_codes = segment_name.split('_')\n",
    "\n",
    "    try:\n",
    "        stop_1_point = Point(utm.from_latlon(*stops.loc[stop_codes[0]][['stop_lat', 'stop_lon']].values)[:2])\n",
    "\n",
    "        stop_2_point = Point(utm.from_latlon(*stops.loc[stop_codes[1]][['stop_lat', 'stop_lon']].values)[:2])\n",
    "    except KeyError:\n",
    "        \n",
    "        segment_cache[segment_name] = line_dist\n",
    "        \n",
    "        return line_dist\n",
    "\n",
    "    for shape_name in trips_to_shapes_dict[route_id]:\n",
    "\n",
    "        shape = shapes_dict[shape_name]\n",
    "        \n",
    "        nearest_1 = nearest_points(shape, stop_1_point)[0]\n",
    "        nearest_2 = nearest_points(shape, stop_2_point)[0]\n",
    "\n",
    "        combined_dist = stop_1_point.distance(nearest_1) + stop_2_point.distance(nearest_2)\n",
    "            \n",
    "#         distances.append(combined_dist)\n",
    "#         lengths.append(np.abs(shape.project(nearest_1) - shape.project(nearest_2)))\n",
    "\n",
    "        # Find the cases where the points are close to the shape\n",
    "        if (combined_dist) <= best_dist:\n",
    "            best_dist = combined_dist\n",
    "            \n",
    "            distances.append(combined_dist)\n",
    "            lengths.append(np.abs(shape.project(nearest_1) - shape.project(nearest_2)))\n",
    "                \n",
    "    if(best_dist >= 15):\n",
    "        \n",
    "        segment_cache[segment_name] = line_dist\n",
    "        \n",
    "        return line_dist\n",
    "\n",
    "    # Take the mean of only those shapes where the points is very close to the shape\n",
    "    new_length = np.mean(np.asarray(lengths)[np.asarray(distances) == best_dist])\n",
    "    \n",
    "    if (new_length == 0) | (new_length > 4000):\n",
    "        \n",
    "        segment_cache[segment_name] = line_dist\n",
    "        \n",
    "        return line_dist\n",
    "\n",
    "    segment_cache[segment_name] = new_length\n",
    "\n",
    "    return segment_cache[segment_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se['route_id'] = routes.set_index('route_short_name').loc[se['publicName']]['route_id'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(\"My Bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se['real_length'] = se.progress_apply(lambda row: find_length_by_name_route(row['segment_name'], row['route_id'], row['line_distance']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se['speed_mph'] = se['real_length'] / se['segment_duration'] * 2.237\n",
    "\n",
    "se['speed_mph_baseline'] = se['real_length'] / se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv(\"messages_with_true_dist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['timeReported'] = pd.to_datetime(messages['timeReported'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages['delta_time'] = messages['timeReported'] - messages.shift(1)['timeReported']\n",
    "# messages['time_seconds'] = messages['delta_time'] / np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages['workCode'] = messages['workCode'].fillna(method=\"ffill\")\n",
    "# messages['tripCode'] = messages['tripCode'].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['dateReported'] = messages['timeReported'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['dateReported'] = pd.to_datetime(messages['dateReported'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performed_work['tripCode'] = performed_work['tripCode'].replace([np.nan, np.inf], -1)\n",
    "\n",
    "# performed_work['tripCode'] = performed_work['tripCode'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = messages.reset_index().merge(performed_work[['workCode', 'tripCode', 'date', 'publicName']],\n",
    "#                             left_on=['workCode', 'tripCode', 'dateReported'], \n",
    "#                             right_on=['workCode', 'tripCode', 'date'],\n",
    "#                            how=\"left\").set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = messages.reset_index().merge(routes[['route_short_name', 'route_id']], \n",
    "#                                         left_on=['publicName'], \n",
    "#                                         right_on=['route_short_name'], \n",
    "#                                         how=\"left\").set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((messages['time_seconds']).values, bins=100, range=(-10, 300));\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fraction under 31 seconds: {np.count_nonzero(messages['time_seconds'] < 31)/messages.shape[0]}\")\n",
    "      \n",
    "print(f\"Between 20 and 31 seconds: {np.count_nonzero((messages['time_seconds'] < 31) & (messages['time_seconds'] > 20))/messages.shape[0]}\")\n",
    "      \n",
    "print(f\"Between 10 and 31 seconds: {np.count_nonzero((messages['time_seconds'] < 31) & (messages['time_seconds'] > 10))/messages.shape[0]}\")\n",
    "      \n",
    "print(f\"Between 5 and 31 seconds: {np.count_nonzero((messages['time_seconds'] < 31) & (messages['time_seconds'] > 5))/messages.shape[0]}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages['delta_lon'] = messages['lon'] - messages.shift(1)['lon']\n",
    "# messages['delta_lat'] = messages['lat'] - messages.shift(1)['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['msg'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def haversine_np(lon1, lat1, lon2, lat2):\n",
    "#     \"\"\"\n",
    "#     Calculate the great circle distance between two points\n",
    "#     on the earth (specified in decimal degrees)\n",
    "\n",
    "#     All args must be of equal length.    \n",
    "\n",
    "#     \"\"\"\n",
    "#     lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "#     dlon = lon2 - lon1\n",
    "#     dlat = lat2 - lat1\n",
    "\n",
    "#     a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "#     c = 2 * np.arcsin(np.sqrt(a))\n",
    "#     km = 6367 * c\n",
    "#     return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages['delta_distance'] = haversine_np(messages['lon'], messages['lat'], messages['lon'].shift(1), messages['lat'].shift(1)) * 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = messages.dropna(subset=['lon', 'lat'])\n",
    "\n",
    "# messages = messages[(messages['time_seconds'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages['lon_next'] = messages['lon'].shift(1)\n",
    "# messages['lat_next'] = messages['lat'].shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_partitions = 20\n",
    "# num_cores = 5\n",
    "\n",
    "# def parallelize_dataframe(df, func, args):\n",
    "#     df_split = np.array_split(df, num_partitions)\n",
    "#     pool = Pool(num_cores)\n",
    "    \n",
    "#     # This line is fidly, we make a list where each item is a tuple of \n",
    "#     # a bit of the dataframe and whatever is passed in as args. \n",
    "#     # Then starmap unpacks that tuple so each copy of func gets it's \n",
    "#     # little bit of the dataframe and the right args to do it's job. \n",
    "#     # All this to avoid globals! \n",
    "#     all_args = [(split,) + args for split in df_split]\n",
    "    \n",
    "#     df = pd.concat(pool.starmap(func, all_args))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages.iloc[1510485]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_dist(messages):\n",
    "\n",
    "#     messages['true_distance'] = messages[['lon', 'lat', \n",
    "#                                           'lon_next', 'lat_next', \n",
    "#                                           'route_id', 'delta_distance'\n",
    "#                                          ]].apply(lambda row: find_length_by_geo_route(*row[['lat', 'lon',\n",
    "#                                                                                           'lat_next', \n",
    "#                                                                                            'lon_next',\n",
    "#                                                                                            'route_id',\n",
    "#                                                                                            'delta_distance'\n",
    "#                                                                                           ]]), axis=1)\n",
    "\n",
    "#     return messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = parallelize_dataframe(messages, find_dist, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages['delta_speed'] = messages['true_distance']/messages['time_seconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages['speed_mph'] = messages['delta_speed'] * 2.237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages.to_csv(\"messages_with_true_dist.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages.to_csv(\"messages_with_true_dist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total: {len(messages)}\")\n",
    "\n",
    "print(f\"changed: {np.count_nonzero(messages['true_distance'] != messages['delta_distance'])}\")\n",
    "      \n",
    "print(f\"is close: {np.count_nonzero(np.isclose(messages['true_distance'], messages['delta_distance']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(messages['delta_distance'], messages['true_distance'], bins=[50,50], range=[[0,800],[0,800]], norm=LogNorm())\n",
    "plt.axis('equal')\n",
    "plt.xlabel(\"straight line distance\")\n",
    "plt.ylabel(\"real distance\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(messages['true_distance'], bins=100, range=(0.1,400));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.count_nonzero(messages['delta_distance']*0.5 >= messages['true_distance'])/len(messages))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total: {len(se)}\")\n",
    "\n",
    "print(f\"changed: {np.count_nonzero(se['real_length'] != se['line_distance']*1000)}\")\n",
    "      \n",
    "print(f\"is close: {np.count_nonzero(np.isclose(se['real_length'], se['line_distance']*1000))}\")\n",
    "\n",
    "plt.hist2d(se['line_distance']*1000, se['real_length'], bins=[50,50], range=[[0,800],[0,800]], norm=LogNorm())\n",
    "plt.axis('equal')\n",
    "plt.xlabel(\"straight line distance\")\n",
    "plt.ylabel(\"real distance\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(messages[(messages['msg'].isin(['inPosition', 'InPointArrive'])) & (messages['time_seconds'] < 31) & (messages['time_seconds'] > 20)]['speed_mph'], bins=100, range=(-10,400));\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(messages[(messages['msg'].isin(['inPosition', 'InPointArrive'])) & (messages['time_seconds'] < 31) & (messages['time_seconds'] > 5)]['speed_mph'], bins=100, range=(-10,400));\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(messages[(messages['msg'].isin(['inPosition', 'InPointArrive'])) & (messages['time_seconds'] < 31) & (messages['time_seconds'] > 5)]['speed_mph'], bins=100, range=(-10,400), label=\"5\");\n",
    "plt.hist(messages[(messages['msg'].isin(['inPosition', 'InPointArrive'])) & (messages['time_seconds'] < 31) & (messages['time_seconds'] > 10)]['speed_mph'], bins=100, range=(-10,400), label=\"10\", alpha=0.5);\n",
    "plt.hist(messages[(messages['msg'].isin(['inPosition', 'InPointArrive'])) & (messages['time_seconds'] < 31) & (messages['time_seconds'] > 20)]['speed_mph'], bins=100, range=(-10,400), label=\"20\", alpha=0.5);\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['prev_speed_mph'] = messages.shift(1)['speed_mph']\n",
    "messages['next_speed_mph'] = messages.shift(-1)['speed_mph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['jitter'] = [False]*messages.shape[0]\n",
    "\n",
    "messages['jitter'] = (messages['speed_mph'] > 75) & \\\n",
    "    (((messages['prev_speed_mph'] > 75) & (messages['next_speed_mph'] < 50)) | \\\n",
    "     ((messages['next_speed_mph'] > 75) & (messages['prev_speed_mph'] < 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = messages[(messages['msg'].isin(['inPosition', 'InPointArrive'])) & (messages['time_seconds'] < 31) & (messages['time_seconds'] > 5) & (messages['jitter'] == False) & (messages['speed_mph'] != np.inf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(filtered['speed_mph'], bins=100, range=(0,75));\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['segment_name'] = filtered['stopCode'].fillna(method=\"ffill\") + \"_\" + filtered['stopCode'].fillna(method=\"bfill\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se['speed_mph'] = se['real_length']/se['segment_duration'] * 2.237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(se['speed_mph'], range=(0, 60), bins=61);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se['mean_speed_mph_by_segment_code_hour_day'] = se['real_length']/se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_data(data, target, cut_point, scale=True):\n",
    "\n",
    "#     cut_point = int(len(data)*test_fraction)\n",
    "    \n",
    "    train_data = data[:cut_point,:].copy()\n",
    "    test_data = data[cut_point:,:].copy()\n",
    "    \n",
    "    test_mask = np.zeros(data.shape[0]).astype(bool)\n",
    "    test_mask[cut_point:] = True\n",
    "\n",
    "    train_target = target[:cut_point].copy()\n",
    "    test_target = target[cut_point:].copy()\n",
    "    \n",
    "    if scale == False:\n",
    "        return train_data, test_data, train_target, test_target\n",
    "    \n",
    "    scaler_target = preprocessing.StandardScaler().fit(train_target[:, None])\n",
    "\n",
    "#     scaler_target = preprocessing.MinMaxScaler().fit(train_target[:, None])\n",
    "\n",
    "\n",
    "    train_target_scaled = scaler_target.transform(train_target[:, None]).astype(np.float32)\n",
    "    test_target_scaled = scaler_target.transform(test_target[:, None]).astype(np.float32)\n",
    "    \n",
    "    data_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    train_data_scaled = data_scaler.fit_transform(train_data).squeeze()\n",
    "    test_data_scaled = data_scaler.transform(test_data).squeeze()\n",
    "\n",
    "    return train_data_scaled, test_data_scaled, train_target_scaled, test_target_scaled, scaler_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_names = se[se_gps_data_hour_prev_next[:,7] >= 10].groupby('segment_name').count().sort_values('date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over4k_names = segment_names[segment_names['date'] > 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(over4k_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_top_gps = se_gps_data_hour_prev_next[se['segment_name'] == over4k_names.index[-1]]\n",
    "\n",
    "plt.hist(just_top_gps[:,7], bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_gps_data_hour_prev_next[np.isnan(se_gps_data_hour_prev_next)] = 0 \n",
    "se['speed_mph'] = se['speed_mph'].replace(np.inf, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures()\n",
    "\n",
    "lr_dict = {}\n",
    "scaler_target_dict = {}\n",
    "scaler_data_dict = {}\n",
    "\n",
    "cut_point = int(len(se)*test_fraction)\n",
    "\n",
    "train_se = se[:cut_point]\n",
    "train_se_gps = se_gps_data_hour_prev_next[:cut_point]\n",
    "\n",
    "pca = PCA(n_components=0.95, svd_solver = 'full', whiten=True)\n",
    "\n",
    "train_se_gps_pca = pca.fit_transform(se_gps_data_hour_prev_next[:cut_point, 1:])\n",
    "\n",
    "mask = np.array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0])\n",
    "\n",
    "for name in tqdm(over4k_names.index):\n",
    "    just_top_gps = train_se_gps[train_se['segment_name'] == name, 1:]\n",
    "    just_top_gps_pca = train_se_gps_pca[train_se['segment_name'] == name]\n",
    "    just_top_se = train_se[train_se['segment_name'] == name]\n",
    "    \n",
    "    just_top_se = just_top_se[just_top_gps[:,6] >= 10]\n",
    "    just_top_gps_pca = just_top_gps_pca[just_top_gps[:,6] >= 10]\n",
    "    just_top_gps = just_top_gps[just_top_gps[:,6] >= 10]\n",
    "\n",
    "    \n",
    "    lr = Ridge()\n",
    "\n",
    "#     scal_data = preprocessing.StandardScaler()\n",
    "    \n",
    "#     scal_target = preprocessing.StandardScaler()\n",
    "    \n",
    "#     scal_data.fit_transform(just_top_gps)\n",
    "    \n",
    "#     svr = SVR(max_iter=1000)\n",
    "\n",
    "#     lr.fit(poly.fit_transform(just_top_gps[:,mask == 1]), just_top_se['speed_mph'])\n",
    "    lr.fit(poly.fit_transform(just_top_gps_pca), just_top_se['speed_mph'])\n",
    "\n",
    "#     svr.fit(scal_data.fit_transform(just_top_gps), scal_target.fit_transform(just_top_se['speed_mph'].values.reshape(-1, 1)).squeeze())\n",
    "    \n",
    "    lr_dict[name] = lr\n",
    "    \n",
    "#     scaler_data_dict[name] = scal_data\n",
    "#     scaler_target_dict[name] = scal_target\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_se = se[cut_point:].copy()\n",
    "test_se_gps = se_gps_data_hour_prev_next[cut_point:]\n",
    "test_se_gps_pca = pca.transform(test_se_gps[:,1:])\n",
    "\n",
    "test_se['pred'] = test_se['real_length'] / test_se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237\n",
    "\n",
    "test_se = test_se.reset_index()\n",
    "\n",
    "for seg_name, seg in tqdm(test_se[['segment_name']].groupby('segment_name')):\n",
    "    if seg_name in lr_dict:\n",
    "        \n",
    "        seg = seg[test_se_gps[seg.index, 7] >= 10]\n",
    "#         test_se.loc[seg.index, 'pred'] = lr_dict[seg_name].predict(poly.fit_transform(test_se_gps[seg.index, 1:][:,mask == 1]))\n",
    "        test_se.loc[seg.index, 'pred'] = lr_dict[seg_name].predict(poly.fit_transform(test_se_gps_pca[seg.index]))\n",
    "#         test_se.loc[seg.index, 'pred'] = scaler_target_dict[seg_name].inverse_transform(lr_dict[seg_name].predict(scaler_data_dict[seg_name].transform(test_se_gps[seg.index, 1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE pred: {np.sqrt(mean_squared_error(test_se['speed_mph'], test_se['pred']))}\")\n",
    "print(f\"RMSE base: {np.sqrt(mean_squared_error(test_se['speed_mph'], test_se['real_length'] / test_se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237))}\")\n",
    "print(f\"RMSE base_median: {np.sqrt(mean_squared_error(test_se['speed_mph'], test_se['real_length'] / test_se['median_durations_by_segment_code_and_hour_and_day'] * 2.237))}\")\n",
    "\n",
    "print(f\"MAE pred: {mean_absolute_error(test_se['speed_mph'], test_se['pred'])}\")\n",
    "print(f\"MAE base: {mean_absolute_error(test_se['speed_mph'], test_se['real_length'] / test_se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237)}\")\n",
    "print(f\"MAE base_median: {mean_absolute_error(test_se['speed_mph'], test_se['real_length'] / test_se['median_durations_by_segment_code_and_hour_and_day'] * 2.237)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Poly, Ridge LR and no standardisation\n",
    "\n",
    "- RMSE pred: 11.423751414830155\n",
    "- RMSE base: 11.622504586790907\n",
    "- RMSE base_median: 11.266602931466238\n",
    "- MAE pred: 4.956844114186245\n",
    "- MAE base: 5.137757484460484\n",
    "- MAE base_median: 4.60521725712747"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA(0.95), Poly, Ridge and no standardisation\n",
    "\n",
    "- RMSE pred: 13.938892847748765\n",
    "- RMSE base: 11.595473016575319\n",
    "- RMSE base_median: 11.237866550948532\n",
    "- MAE pred: 5.0227073658086265\n",
    "- MAE base: 5.127412926002986\n",
    "- MAE base_median: 4.595154570229731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA(0.95, whiten=True), Poly, Ridge and no standardisation\n",
    "\n",
    "- RMSE pred: 11.40829141469984\n",
    "- RMSE base: 11.595473016575319\n",
    "- RMSE base_median: 11.237866550948532\n",
    "- MAE pred: 4.975194723303201\n",
    "- MAE base: 5.127412926002986\n",
    "- MAE base_median: 4.595154570229731\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(baseline_array_cum, \n",
    "     actual_array_cum, \n",
    "     baseline_median_array_cum, \n",
    "     baseline_pass_count, \n",
    "     pass_fraction, \n",
    "     baseline_median_pass_count, \n",
    "     median_pass_fraction) = calc_baseline_and_actual(test_se)\n",
    "\n",
    "predict_array_linear_cum, _, pass_fraction = calc_prediction_cum_journeys(test_se, test_se['pred'], baseline_median_array_cum, actual_array_cum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array_linear_cum, _, pass_fraction = calc_prediction_cum_journeys(test_se, test_se['pred'], baseline_array_cum, actual_array_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(make_accuracy_matrix_minutes(predict_array_linear_cum, actual_array_cum)[0][0,:], label=\"Linear\")\n",
    "plt.plot(make_accuracy_matrix_minutes(baseline_array_cum, actual_array_cum)[0][0,:], label=\"means\")\n",
    "plt.plot(make_accuracy_matrix_minutes(baseline_median_array_cum, actual_array_cum)[0][0,:], label=\"medians\")\n",
    "plt.plot(make_accuracy_matrix_minutes((baseline_median_array_cum + baseline_array_cum)/2, actual_array_cum)[0][0,:], label=\"mean of aves\")\n",
    "plt.xlim(0,25)\n",
    "plt.ylim(0,60)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_top_gps = se_gps_data_hour_prev_next[se['segment_name'] == '1290BOB20412_1290BOB20410']\n",
    "just_top_se = se[se['segment_name'] == '1290BOB20412_1290BOB20410']\n",
    "\n",
    "data = np.hstack((se_gps_data_hour_prev_next[:,1:], se[['real_length', \n",
    "                                                            'direction_degrees', \n",
    "                                                            \"to_centre_dist\",\n",
    "                                                           'rain',\n",
    "                                                           'arrival_hour',\n",
    "                                                           'arrival_day']].values))\n",
    "\n",
    "data = data[se['segment_name'] == '1290BOB20412_1290BOB20410']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"percent invalid just_top_gps: {np.count_nonzero(np.isnan(just_top_gps) |~np.isfinite(just_top_gps))/just_top_gps.size*100}\")\n",
    "print(f\"percent invalid just_top_se['speed_mph']: {np.count_nonzero(np.isnan(just_top_se['speed_mph']) |~np.isfinite(just_top_se['speed_mph']))/just_top_se['speed_mph'].size*100}\")\n",
    "print(f\"percent invalid just_top_se['speed_mph']: {np.count_nonzero(np.isnan(data) |~np.isfinite(data))/data.size*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_top_gps[np.isnan(just_top_gps)] = 0\n",
    "data[np.isnan(data)] = 0\n",
    "just_top_se['speed_mph'] = just_top_se['speed_mph'].replace(np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(just_top_gps[:,7], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_point = int(len(just_top_se)*test_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(baseline_array_cum, \n",
    "     actual_array_cum, \n",
    "     baseline_median_array_cum, \n",
    "     baseline_pass_count, \n",
    "     pass_fraction, \n",
    "     baseline_median_pass_count, \n",
    "     median_pass_fraction) = calc_baseline_and_actual(just_top_se[cut_point:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures()\n",
    "\n",
    "# (train_data_scaled, test_data_scaled, \n",
    "#  train_target_scaled, test_target_scaled, \n",
    "#  scaler_target) = prep_training_data(poly.fit_transform(just_top_gps), just_top_se['speed_mph'].values, cut_point)\n",
    "\n",
    "(train_data_scaled, test_data_scaled, \n",
    " train_target_scaled, test_target_scaled) = prep_training_data(poly.fit_transform(just_top_gps), just_top_se['speed_mph'].values, cut_point, False)\n",
    "\n",
    "test_target_truth = scaler_target.inverse_transform(test_target_scaled).squeeze()\n",
    "\n",
    "lr = LinearRegression()\n",
    "ar = ARDRegression()\n",
    "# lr = Lasso()\n",
    "\n",
    "lr.fit(train_data_scaled, train_target_scaled)\n",
    "\n",
    "test_y_scaled = lr.predict(test_data_scaled)\n",
    "\n",
    "# test_y = scaler_target.inverse_transform(test_y_scaled).squeeze()\n",
    "\n",
    "test_y = test_y_scaled\n",
    "\n",
    "# test_target_truth = scaler_target.inverse_transform(test_target_scaled).squeeze()\n",
    "\n",
    "test_target_truth = test_target_scaled\n",
    "\n",
    "plt.scatter(test_target_truth, test_y, marker=\".\", alpha=0.3)\n",
    "plt.xlabel(\"true target (mph)\")\n",
    "plt.ylabel(\"prediction target (mph)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlim(-5,100)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(test_target_truth, just_top_se[cut_point:]['real_length'] / just_top_se[cut_point:]['mean_durations_by_segment_code_and_hour_and_day'] * 2.237, marker=\".\", alpha=0.3)\n",
    "plt.xlabel(\"true target (mph)\")\n",
    "plt.ylabel(\"prediction target (mph)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlim(-5,100)\n",
    "\n",
    "print(f\"RMSE pred: {np.sqrt(mean_squared_error(test_target_truth, test_y))}\")\n",
    "print(f\"RMSE base: {np.sqrt(mean_squared_error(test_target_truth, just_top_se[cut_point:]['real_length'] / just_top_se[cut_point:]['mean_durations_by_segment_code_and_hour_and_day'] * 2.237))}\")\n",
    "\n",
    "print(f\"MAE pred: {mean_absolute_error(test_target_truth, test_y)}\")\n",
    "print(f\"MAE base: {mean_absolute_error(test_target_truth, just_top_se[cut_point:]['real_length'] / just_top_se[cut_point:]['mean_durations_by_segment_code_and_hour_and_day'] * 2.237)}\")\n",
    "\n",
    "predict_array_linear_cum, _, pass_fraction = calc_prediction_cum_journeys(just_top_se[cut_point:], test_y, baseline_array_cum, actual_array_cum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(make_accuracy_matrix_minutes(predict_array_NN_cum, actual_array_cum)[0][1,:], label=\"NN\")\n",
    "plt.plot(make_accuracy_matrix_minutes(baseline_array_cum, actual_array_cum)[0][1,:], label=\"means\")\n",
    "plt.plot(make_accuracy_matrix_minutes(baseline_median_array_cum, actual_array_cum)[0][1,:], label=\"medians\")\n",
    "plt.plot(make_accuracy_matrix_minutes((baseline_median_array_cum + baseline_array_cum)/2, actual_array_cum)[0][1,:], label=\"mean of aves\")\n",
    "plt.xlim(0,25)\n",
    "plt.ylim(0,60)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_names = np.array(names).squeeze()[np.argsort(np.abs(lr.coef_))].squeeze()\n",
    "\n",
    "ordered_coef = np.array(lr.coef_).squeeze()[np.argsort(np.abs(lr.coef_))].squeeze()\n",
    "\n",
    "display(list(zip(ordered_names, ordered_coef))[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model(input_shape, dropout, NN, use_linear):\n",
    "\n",
    "    # with help from: https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "    # Headline input: meant to receive road time series.\n",
    "    main_input = Input(shape=[input_shape], dtype=\"float32\", name=\"input\")\n",
    "\n",
    "    for index, layer in enumerate(NN):\n",
    "        if(index == 0):\n",
    "            x = Dense(layer, activation=\"relu\")(main_input)\n",
    "            x = Dropout(rate=dropout)(x)\n",
    "        else:\n",
    "            x = Dense(layer, activation=\"relu\")(x)\n",
    "            x = Dropout(rate=dropout)(x)\n",
    "\n",
    "#     # We stack a deep densely-connected network on top\n",
    "#     x = Dense(128, activation=\"relu\")(main_input)\n",
    "#     x = Dropout(rate=dropout)(x)\n",
    "#     x = Dense(64, activation=\"relu\")(x)\n",
    "#     x = Dropout(rate=dropout)(x)\n",
    "#     x = Dense(32, activation=\"relu\")(x)\n",
    "#     x = Dropout(rate=dropout)(x)\n",
    "#     x = Dense(32, activation=\"relu\")(x)\n",
    "#     x = Dropout(rate=dropout)(x)\n",
    "#     x = Dense(12, activation=\"relu\")(x)\n",
    "#     x = Dropout(rate=dropout)(x)\n",
    "\n",
    "    # And finally we add the main output layer\n",
    "    if use_linear:\n",
    "        main_output = Dense(1, activation=\"linear\", name=\"main_output\")(x)\n",
    "    else: \n",
    "        main_output = Dense(1, activation=\"tanh\", name=\"main_output\")(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[main_input], outputs=[main_output]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((se_gps_data_hour_prev_next[:,1:], se[['real_length', \n",
    "                                                            'direction_degrees', \n",
    "                                                            \"to_centre_dist\",\n",
    "                                                           'rain',\n",
    "                                                           'arrival_hour',\n",
    "                                                           'arrival_day']].values))\n",
    "\n",
    "df_cors = pd.DataFrame(data=np.hstack((data, se[['speed_mph']].values)),\n",
    "                    columns=['slow_fraction', 'fast_hmean', 'fast_mean', 'total_mean',\n",
    "                                'clipped_hmean','clipped_mean','freq_total',\n",
    "                                'freq_fast','freq_clipped', \n",
    "                               'fast_median', 'fast_75th percential',\n",
    "                              'total_median', 'total_75th percential', 'max', \n",
    "                             'real_length', 'direction_degrees', \"to_centre_dist\",\n",
    "                                                           'rain',\n",
    "                                                           'arrival_hour',\n",
    "                                                           'arrival_day','true_speed'])\n",
    "\n",
    "cors = df_cors[df_cors['freq_fast'] >= 10].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For version 3\n",
    "\n",
    "cors['true_speed'].abs().sort_values().index[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(se_gps_data_hour_prev_next[:,1], bins=50, range=(0,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(se_gps_data_hour_prev_next[:,4], bins=50, range=(0,50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(se_gps_data_hour_prev_next[:,13], bins=50, range=(0,50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(se['speed_mph'], bins=50, range=(0,50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(se['speed_mph'], se_gps_data_hour_prev_next[:,13], range=[[0,50],[0,30]], bins=[50,50]);\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(filtered['speed_mph'], bins=50, range=(3,50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_valid_mask(freq):\n",
    "\n",
    "    valid_mask = (se_gps_data_hour_prev_next[:,9] >= freq) & \\\n",
    "    (se['speed_mph'] != np.inf) & \\\n",
    "    (~np.isnan(se_gps_data_hour_prev_next[:,4])) & \\\n",
    "    (~np.isnan(se_gps_data_hour_prev_next[:,10])) & \\\n",
    "    (se_gps_data_hour_prev_next[:,13] != 0)\n",
    "    \n",
    "    return valid_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_baselines(valid_mask):\n",
    "#     (baseline_array_cum, \n",
    "#      actual_array_cum, \n",
    "#      baseline_median_array_cum, \n",
    "#      baseline_pass_count, \n",
    "#      pass_fraction, \n",
    "#      baseline_median_pass_count, \n",
    "#      median_pass_fraction) =  calc_baseline_and_actual(se[int(len(se)*test_fraction):])\n",
    "\n",
    "#     return (baseline_array_cum, \n",
    "#          actual_array_cum, \n",
    "#          baseline_median_array_cum, \n",
    "#          baseline_pass_count, \n",
    "#          pass_fraction, \n",
    "#          baseline_median_pass_count, \n",
    "#          median_pass_fraction)\n",
    "\n",
    "# calc_baseline_and_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 slow_fraction\n",
    "# 1 fast_hmean\n",
    "# 2 fast_mean\n",
    "# 3 total_mean\n",
    "# 4 clipped_hmean\n",
    "# 5 clipped_mean\n",
    "# 6 freq_total\n",
    "# 7 freq_fast\n",
    "# 8 freq_clipped\n",
    "# 9 fast_median\n",
    "# 10 fast_75th percential\n",
    "# 11 total_median\n",
    "# 12 total_75th percential\n",
    "# 13 max\n",
    "\n",
    "overlaps_dict = {}\n",
    "overlaps_dict[0] = []\n",
    "overlaps_dict[1] = [4, 5, 11, 9, 12]\n",
    "overlaps_dict[2] = [3, 10, 9]\n",
    "overlaps_dict[3] = [2, 10, 12]\n",
    "overlaps_dict[4] = [1, 5]\n",
    "overlaps_dict[5] = [1, 4]\n",
    "overlaps_dict[6] = [7, 8]\n",
    "overlaps_dict[7] = [6, 8]\n",
    "overlaps_dict[8] = [6, 7]\n",
    "overlaps_dict[9] = [1, 2, 10, 12]\n",
    "overlaps_dict[10] = [2, 9, 3]\n",
    "overlaps_dict[11] = [1, 12]\n",
    "overlaps_dict[12] = [1, 3, 9, 11]\n",
    "overlaps_dict[13] = []\n",
    "overlaps_dict[14] = []\n",
    "overlaps_dict[15] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overlap_mask(width = 16):\n",
    "    overlaps_mask = np.zeros(width)\n",
    "\n",
    "    overlaps_mask[:] = -1\n",
    "    \n",
    "    overlaps_mask[11] = 1 # 11 total_median\n",
    "    overlaps_mask[4] = 1 # 4 clipped_hmean\n",
    "    overlaps_mask[3] = 1 # 3 total_mean\n",
    "    \n",
    "    while(np.isin([-1], overlaps_mask).any()):\n",
    "        options = np.where(overlaps_mask < 0)[0]\n",
    "    #     print(options)\n",
    "        i = np.random.randint(len(options))\n",
    "#         print(i)\n",
    "        overlaps_mask[options[i]] = 1\n",
    "        overlaps_mask[overlaps_dict[options[i]]] = 0\n",
    "\n",
    "    return overlaps_mask.astype(bool)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_overlap_mask(freq, data, columns):\n",
    "    \n",
    "#     data = np.hstack((se_gps_data_hour_prev_next[:,1:], se[['real_length', \n",
    "#                                                             'direction_degrees', \n",
    "#                                                             \"to_centre_dist\",\n",
    "#                                                            'rain',\n",
    "#                                                            'arrival_hour',\n",
    "#                                                            'arrival_day']].values))\n",
    "\n",
    "    df_cors = pd.DataFrame(data=np.hstack((data, se[['speed_mph']].values)),\n",
    "                        columns=columns)\n",
    "\n",
    "    cors = df_cors[df_cors['freq_fast'] >= freq].corr()\n",
    "    \n",
    "    overlaps_dict = {}\n",
    "\n",
    "    abs_cor_top_75th = np.percentile(np.abs(cors.values), 75)\n",
    "\n",
    "    for col_idx, column in enumerate(cors.columns):\n",
    "\n",
    "        if column == 'true_speed':\n",
    "            continue\n",
    "\n",
    "        overlaps_dict[column] = []\n",
    "\n",
    "        for row_idx, (name, value) in enumerate(cors[column].iteritems()):\n",
    "            if np.abs(value) > abs_cor_top_75th and column != name:\n",
    "    #             print(f\"{column} -> {name}: {value}\")\n",
    "                overlaps_dict[column].append(row_idx)\n",
    "        \n",
    "    overlap_mask = np.empty(len(cors['true_speed']))\n",
    "\n",
    "    overlap_mask[:] = -1\n",
    "\n",
    "    for best in cors['true_speed'].abs().sort_values().index[::-1]:\n",
    "\n",
    "        if(best == \"true_speed\"):\n",
    "            continue\n",
    "\n",
    "        best_idx = cors.columns.get_loc(best)\n",
    "\n",
    "        if overlap_mask[best_idx] == -1:\n",
    "            overlap_mask[best_idx] = 1\n",
    "            overlap_mask[overlaps_dict[best]] = 0\n",
    "\n",
    "\n",
    "    return overlap_mask[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speeds = se['speed_mph'][:int(len(se)*test_fraction)]\n",
    "\n",
    "mean_speed = np.mean(train_speeds[np.isfinite(train_speeds)])\n",
    "print(mean_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(baseline_array_cum, \n",
    "     actual_array_cum, \n",
    "     baseline_median_array_cum, \n",
    "     baseline_pass_count, \n",
    "     pass_fraction, \n",
    "     baseline_median_pass_count, \n",
    "     median_pass_fraction) = calc_baseline_and_actual(se[int(len(se)*test_fraction):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(freq, dropout, loss, NN, use_linear = False):\n",
    "    \n",
    "    valid_mask = make_valid_mask(freq)\n",
    "    \n",
    "#     data = np.hstack((se_gps_data_hour_prev_next[:,1:], se[['real_length', \n",
    "#                                                             'direction_degrees', \n",
    "#                                                             \"to_centre_dist\",\n",
    "#                                                            'rain',\n",
    "#                                                            'arrival_hour',\n",
    "#                                                            'arrival_day'\n",
    "#                                                            ]].values))\n",
    "\n",
    "#     overlaps_mask = best_overlap_mask(freq, data, ['slow_fraction', 'fast_hmean', 'fast_mean', 'total_mean',\n",
    "#                                     'clipped_hmean','clipped_mean','freq_total',\n",
    "#                                     'freq_fast','freq_clipped', \n",
    "#                                    'fast_median', 'fast_75th percential',\n",
    "#                                   'total_median', 'total_75th percential', 'max', \n",
    "#                                  'real_length', 'direction_degrees', \"to_centre_dist\",\n",
    "#                                                            'rain',\n",
    "#                                                            'arrival_hour',\n",
    "#                                                            'arrival_day','true_speed'])\n",
    "\n",
    "\n",
    "    data = se_gps_data_hour_prev_next[:,1:]\n",
    "\n",
    "    overlaps_mask = best_overlap_mask(freq, data, ['slow_fraction', 'fast_hmean', 'fast_mean', 'total_mean',\n",
    "                                    'clipped_hmean','clipped_mean','freq_total',\n",
    "                                    'freq_fast','freq_clipped', \n",
    "                                   'fast_median', 'fast_75th percential',\n",
    "                                  'total_median', 'total_75th percential', 'max', \n",
    "                                    'true_speed'])\n",
    "\n",
    "    \n",
    "    # where on the reduced (valid mask) data is the right place to cut so that when it comes back\n",
    "    # it's exactly test_fraction from the start\n",
    "    cut_point_valid = np.count_nonzero(valid_mask[:int(len(se)*test_fraction)])\n",
    "    \n",
    "#     speeds_mph_simple = se['real_length'] / se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237\n",
    "    \n",
    "#     (train_data_scaled, test_data_scaled, \n",
    "#      train_target_scaled_baseline, test_target_scaled_baseline, \n",
    "#      scaler_target_baseline) = prep_training_data(data[valid_mask,:][:, (overlaps_mask==1)], speeds_mph_simple[valid_mask], cut_point_valid)\n",
    "\n",
    "\n",
    "    (train_data_scaled, test_data_scaled, \n",
    "     train_target_scaled, test_target_scaled, \n",
    "     scaler_target) = prep_training_data(data[valid_mask,:][:, (overlaps_mask==1)], se[valid_mask]['speed_mph'], cut_point_valid)\n",
    "\n",
    "    print(overlaps_mask)\n",
    "    \n",
    "    model = create_simple_model(\n",
    "            (train_data_scaled.shape[1]),\n",
    "            dropout, NN, use_linear)\n",
    "\n",
    "    Path(f\"GPS_models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"GPS_models/freq_{freq}_mask_{overlaps_mask}_simple_[{'_'.join(list(map(str, NN)))}]_{loss}_{dropout}_{use_linear}.h5\",\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # model.compile(optimizer=\"rmsprop\", loss=\"mean_absolute_error\")\n",
    "#     model.compile(optimizer=\"rmsprop\", loss=\"logcosh\")\n",
    "    model.compile(optimizer=\"rmsprop\", loss=loss)\n",
    "    model.fit(\n",
    "        train_data_scaled,\n",
    "        train_target_scaled,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks_list,\n",
    "        batch_size=256,\n",
    "        validation_data=(test_data_scaled, test_target_scaled),\n",
    "    )\n",
    "\n",
    "    test_y_scaled = model.predict(test_data_scaled)\n",
    "\n",
    "    test_y = (se['real_length'] / se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237)[int(len(se)*test_fraction):]\n",
    "\n",
    "    test_y[valid_mask[int(len(se)*test_fraction):]] = scaler_target.inverse_transform(test_y_scaled)\n",
    "    \n",
    "    test_target_truth = se['speed_mph'][int(len(se)*test_fraction):]\n",
    "    \n",
    "    test_target_truth[~np.isfinite(test_target_truth)] = mean_speed\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(test_target_truth, test_y))\n",
    "    \n",
    "    MAE = mean_absolute_error(test_target_truth, test_y)\n",
    "    \n",
    "    predict_array_NN_cum, _, pass_fraction = calc_prediction_cum_journeys(se[int(len(se)*test_fraction):], test_y, baseline_array_cum, actual_array_cum)\n",
    "\n",
    "    return overlaps_mask, pass_fraction, RMSE, MAE, test_y, predict_array_NN_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_KNN(freq=10, k=5):\n",
    "    \n",
    "    valid_mask = make_valid_mask(freq)\n",
    "    \n",
    "    data = se_gps_data_hour_prev_next[:,1:]\n",
    "\n",
    "    overlaps_mask = best_overlap_mask(freq, data, ['slow_fraction', 'fast_hmean', 'fast_mean', 'total_mean',\n",
    "                                    'clipped_hmean','clipped_mean','freq_total',\n",
    "                                    'freq_fast','freq_clipped', \n",
    "                                   'fast_median', 'fast_75th percential',\n",
    "                                  'total_median', 'total_75th percential', 'max', \n",
    "                                    'true_speed'])\n",
    "\n",
    "    \n",
    "    # where on the reduced (valid mask) data is the right place to cut so that when it comes back\n",
    "    # it's exactly test_fraction from the start\n",
    "    cut_point_valid = np.count_nonzero(valid_mask[:int(len(se)*test_fraction)])\n",
    "    \n",
    "#     speeds_mph_simple = se['real_length'] / se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237\n",
    "    \n",
    "#     (train_data_scaled, test_data_scaled, \n",
    "#      train_target_scaled_baseline, test_target_scaled_baseline, \n",
    "#      scaler_target_baseline) = prep_training_data(data[valid_mask,:][:, (overlaps_mask==1)], speeds_mph_simple[valid_mask], cut_point_valid)\n",
    "\n",
    "\n",
    "    (train_data_scaled, test_data_scaled, \n",
    "     train_target_scaled, test_target_scaled, \n",
    "     scaler_target) = prep_training_data(data[valid_mask,:][:, (overlaps_mask==1)], se[valid_mask]['speed_mph'], cut_point_valid)\n",
    "\n",
    "    neigh = KNeighborsRegressor(n_neighbors=k, weights=\"distance\")\n",
    "    neigh.fit(train_data_scaled, train_target_scaled) \n",
    "\n",
    "    test_y_scaled = neigh.predict(test_data_scaled)\n",
    "\n",
    "    test_y = (se['real_length'] / se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237)[int(len(se)*test_fraction):]\n",
    "\n",
    "    test_y[valid_mask[int(len(se)*test_fraction):]] = scaler_target.inverse_transform(test_y_scaled)\n",
    "    \n",
    "    test_target_truth = se['speed_mph'][int(len(se)*test_fraction):]\n",
    "    \n",
    "    test_target_truth[~np.isfinite(test_target_truth)] = mean_speed\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(test_target_truth, test_y))\n",
    "    \n",
    "    MAE = mean_absolute_error(test_target_truth, test_y)\n",
    "    \n",
    "    predict_array_NN_cum, _, pass_fraction = calc_prediction_cum_journeys(se[int(len(se)*test_fraction):], test_y, baseline_array_cum, actual_array_cum)\n",
    "\n",
    "    return overlaps_mask, pass_fraction, RMSE, MAE, test_y, predict_array_NN_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, test_y, predict_array_KNN_cum = learn_KNN(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, test_y, predict_array_NN_cum = learn(10, 0.0, \"logcosh\", [64,32,12], False)\n",
    "\n",
    "# Draw real vs streight length for messages - Yes\n",
    "# Try using straight lengths for the messages instead - Bad\n",
    "# Try using a linear instead of tanh activation for the last layer - Worse\n",
    "# Try predicting the mean and then still testing against true, for various networks\n",
    "# Try to find one bus route where this works really well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"GPS_run_3.csv\", \"a+\") as f: \n",
    "    f.write(\"freq_threshold,dropout,loss,overlap_mask,NN,use_linear,pass_fraction,RMSE,MAE\\n\")\n",
    "    for runs in range(5):\n",
    "        for dropout in [0,0.2,0.5]:\n",
    "            for loss in ['logcosh', 'mean_absolute_error', 'mean_squared_error']:\n",
    "                for NN in [[64,32,12]]: #[[32,12,6], [64,32,12], [128,64,32,32,12]]:\n",
    "                    for freq in [3,10,15,25]:\n",
    "                        for use_linear in [True, False]:\n",
    "                    \n",
    "                            overlaps_mask, pass_fraction, RMSE, MAE, test_y = learn(freq, dropout, loss, NN, use_linear)\n",
    "                            f.write(f\"{freq},{dropout},{loss},{'_'.join(list(map(str,overlaps_mask)))},{'_'.join(list(map(str,NN)))},{use_linear},{pass_fraction},{RMSE},{MAE}\\n\")\n",
    "                            f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(make_accuracy_matrix_minutes(predict_array_KNN_cum, actual_array_cum)[0][1,:], label=\"NN\")\n",
    "plt.plot(make_accuracy_matrix_minutes(baseline_array_cum, actual_array_cum)[0][1,:], label=\"means\")\n",
    "plt.plot(make_accuracy_matrix_minutes(baseline_median_array_cum, actual_array_cum)[0][1,:], label=\"medians\")\n",
    "plt.plot(make_accuracy_matrix_minutes((baseline_median_array_cum + baseline_array_cum)/2, actual_array_cum)[0][1,:], label=\"mean of aves\")\n",
    "plt.xlim(0,25)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_y, bins=100, label=\"pred\", density=True, range=(-10,100));\n",
    "plt.hist(se['speed_mph'][int(len(se)*test_fraction):], bins=100, label=\"truth\", alpha=0.5, density=True, range=(-10,100));\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds_mph_simple = se['real_length'] / se['mean_durations_by_segment_code_and_hour_and_day'] * 2.237\n",
    "\n",
    "se_min = se[['speed_mph', 'publicName', 'to_centre_dist', 'real_length', 'mean_durations_by_segment_code_and_hour_and_day']][int(len(se)*test_fraction):].copy()\n",
    "\n",
    "se_min['baseline_speed'] = se_min['real_length'] / se_min['mean_durations_by_segment_code_and_hour_and_day'] * 2.237\n",
    "\n",
    "se_min['test_y'] = test_y\n",
    "\n",
    "routes = []\n",
    "pred_MSE = []\n",
    "pred_RMSE = []\n",
    "base_MSE = []\n",
    "base_RMSE = []\n",
    "freq = []\n",
    "mean_dist = []\n",
    "\n",
    "for route_name, route in se_min.groupby('publicName'):\n",
    "    \n",
    "    if route_name in schools:\n",
    "        continue\n",
    "        \n",
    "    routes.append(route_name)\n",
    "    pred_MSE.append(mean_absolute_error(route['speed_mph'], route['test_y']))\n",
    "    pred_RMSE.append(np.sqrt(mean_squared_error(route['speed_mph'], route['test_y'])))\n",
    "    base_MSE.append(mean_absolute_error(route['speed_mph'], route['baseline_speed']))\n",
    "    base_RMSE.append(np.sqrt(mean_squared_error(route['speed_mph'], route['baseline_speed'])))\n",
    "    freq.append(len(route))\n",
    "    \n",
    "results = pd.DataFrame(index=routes)\n",
    "results['pred_MSE'] = pred_MSE\n",
    "results['base_MSE'] = base_MSE\n",
    "results['pred_RMSE'] = pred_RMSE\n",
    "results['base_RMSE'] = base_RMSE\n",
    "results['freq'] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('pred_RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = ['46', '50', '60', '81', '85', '86', '87', '87A', '88', '111']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(se['publicName'].isin(schools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_target_truth = se['speed_mph'][int(len(se[valid_mask])*test_fraction):]\n",
    "\n",
    "test_target_truth = scaler_target.inverse_transform(test_target_scaled).squeeze()\n",
    "\n",
    "print(np.sqrt(mean_squared_error(test_target_truth, test_y)))\n",
    "\n",
    "plt.hist2d(test_target_truth, test_y.squeeze(), range=[[0,30],[0,30]], bins=[30,30])\n",
    "plt.xlabel(\"true target (mph)\")\n",
    "plt.ylabel(\"prediction target (mph)\")\n",
    "plt.ylim(-5,35)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array_RNN_cum, _, pass_fraction = calc_prediction_cum_journeys(se[valid_mask][int(len(se[valid_mask])*test_fraction):], test_y.squeeze(), baseline_array_cum, actual_array_cum)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_in_x_percent(predict, actual, threshold):\n",
    "    \n",
    "    if np.count_nonzero(~np.isnan(actual)) == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    threshold = threshold/100\n",
    "    \n",
    "    mask = (~np.isnan(predict) & ~np.isnan(actual))\n",
    "    \n",
    "    pass_count = np.count_nonzero((predict[mask] < actual[mask] * (1 + threshold)) & (predict[mask] > actual[mask] * (1-threshold)))\n",
    "    \n",
    "    over_count = np.count_nonzero(predict[mask] > actual[mask] * (1+threshold))\n",
    "    \n",
    "    under_count = np.count_nonzero(predict[mask] < actual[mask] * (1-threshold))\n",
    "    \n",
    "    pass_percent = pass_count/np.count_nonzero(mask) * 100\n",
    "    \n",
    "    if over_count + under_count == 0:\n",
    "        drift = 0.5\n",
    "    else:\n",
    "        drift = over_count / (over_count + under_count)\n",
    "    \n",
    "    return pass_percent, drift\n",
    "\n",
    "def make_accuracy_matrix_minutes(predict, actual, max_threshold = 50):\n",
    "\n",
    "    actual_ints = np.array(actual/60).astype(int)\n",
    "    \n",
    "    rows = int(max_threshold/10)\n",
    "    \n",
    "    max_a = np.nanmax(actual)/60\n",
    "\n",
    "    accuracies_table = np.empty((rows, int(max_a)))\n",
    "    drift_table = np.empty((rows, int(max_a)))\n",
    "    frequency = np.empty(int(max_a))\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "    for i in range(int(max_a)):\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        mask = (actual_ints == i)\n",
    "        \n",
    "        frequency[i] = np.count_nonzero(mask)\n",
    "        \n",
    "        for j in range(1, rows+1):\n",
    "            accuracy, drift = percent_in_x_percent(predict[mask], actual[mask], j * 10)\n",
    "            accuracies_table[j-1,i] = accuracy\n",
    "            drift_table[j-1, i] = drift\n",
    "\n",
    "    return accuracies_table, frequency, drift_table\n",
    "\n",
    "def show_accuracy_minutes(predict, actual, title):\n",
    "    results, frequency, drift = make_accuracy_matrix_minutes(predict, actual)\n",
    "    \n",
    "    for i in range(results.shape[0]):\n",
    "        plt.plot(results[i,:], label=f\"{(i+1)*10}%\")\n",
    "        \n",
    "        \n",
    "    plt.xlabel(\"minutes ahead\")\n",
    "    plt.ylabel(\"percentage within threshold\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlim(0,20)\n",
    "    plt.ylim(0,100)\n",
    "    plt.gca().yaxis.grid(True, linewidth=\"0.2\")\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(drift[0,:], label=\"fraction over\", linestyle=\":\")\n",
    "    ax2.set_ylim(0,1)\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://stackoverflow.com/questions/51597849/padding-a-numpy-array-with-offsets-for-each-data-column\n",
    "\n",
    "def create_padded_array(a, row_start, n_rows):\n",
    "    r = np.arange(n_rows)[:,None]\n",
    "    row_start = np.asarray(row_start)\n",
    "    mask = (r >= row_start) & (r < row_start+a.shape[0])\n",
    "\n",
    "    out = np.zeros(mask.shape, dtype=a.dtype)\n",
    "    out[:] = np.nan\n",
    "    out.T[mask.T] = a.ravel('F')\n",
    "    return out   \n",
    "\n",
    "def create_triangle(input_array, max_width=70):\n",
    "    \n",
    "    filled_values = np.empty((input_array.shape[0],70)).astype(float)\n",
    "    \n",
    "    filled_values[:] = input_array[:,None]\n",
    "    \n",
    "    return create_padded_array(filled_values, list(range(70)), input_array.shape[0]+71)[:input_array.shape[0],:]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baseline_and_actual(data):\n",
    "    \n",
    "    se_min = data.copy()\n",
    "    \n",
    "    baseline_array = np.empty((se_min.shape[0],70)).astype(float)\n",
    "    baseline_array[:] = np.nan\n",
    "    \n",
    "    actual_array = np.empty((se_min.shape[0],70)).astype(float)\n",
    "    actual_array[:] = np.nan\n",
    "    \n",
    "    baseline_median_array = np.empty((se_min.shape[0],70)).astype(float)\n",
    "    baseline_median_array[:] = np.nan\n",
    "    \n",
    "    se_min = se_min.reset_index(drop=True)\n",
    "    \n",
    "    runs = se_min.groupby(['date','workid'])\n",
    "    \n",
    "    actual_index = se_min.columns.get_loc(\"segment_duration\")\n",
    "    baseline_index = se_min.columns.get_loc(\"mean_durations_by_segment_code_and_hour_and_day\")\n",
    "    baseline_median_index = se_min.columns.get_loc(\"median_durations_by_segment_code_and_hour_and_day\")\n",
    "\n",
    "    for name, run in runs:\n",
    "        run = run.sort_values(\"actualArrival\")\n",
    "        \n",
    "        baseline_array[run.index,:] = create_triangle(run.iloc[:, baseline_index])\n",
    "        actual_array[run.index,:] = create_triangle(run.iloc[:, actual_index])\n",
    "        baseline_median_array[run.index,:] = create_triangle(run.iloc[:, baseline_median_index])\n",
    "            \n",
    "    baseline_array_cum = np.cumsum(baseline_array, axis=1)\n",
    "    actual_array_cum = np.cumsum(actual_array, axis=1)\n",
    "    baseline_median_array_cum = np.cumsum(baseline_median_array, axis=1)\n",
    "    \n",
    "    actual_array_cum = np.clip(actual_array_cum, 0, 2*60*60)\n",
    "    \n",
    "    first_20mins_mask = actual_array_cum < 20*60\n",
    "    \n",
    "    baseline_pass_count = np.count_nonzero((baseline_array_cum[first_20mins_mask] < actual_array_cum[first_20mins_mask] * 1.1) & \\\n",
    "                    (baseline_array_cum[first_20mins_mask] > actual_array_cum[first_20mins_mask] * 0.9))\n",
    "    \n",
    "    pass_fraction = baseline_pass_count/np.count_nonzero(first_20mins_mask)\n",
    "    \n",
    "    baseline_median_pass_count = np.count_nonzero((baseline_median_array_cum[first_20mins_mask] < actual_array_cum[first_20mins_mask] * 1.1) & \\\n",
    "                    (baseline_median_array_cum[first_20mins_mask] > actual_array_cum[first_20mins_mask] * 0.9))\n",
    "    \n",
    "    median_pass_fraction = baseline_median_pass_count/np.count_nonzero(first_20mins_mask)\n",
    "    \n",
    "    return baseline_array_cum, actual_array_cum, baseline_median_array_cum, baseline_pass_count, pass_fraction, baseline_median_pass_count, median_pass_fraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prediction_cum_journeys(data, speeds_mph, baseline_array_cum, actual_array_cum):\n",
    "    \n",
    "    se_min = data.copy()\n",
    "      \n",
    "    se_min['prediction'] = se_min['real_length'] / speeds_mph * 2.237\n",
    "    \n",
    "    predict_array = np.empty((se_min.shape[0],70)).astype(float)\n",
    "    predict_array[:] = np.nan\n",
    "    \n",
    "    se_min = se_min.reset_index(drop=True)\n",
    "    \n",
    "    runs = se_min.groupby(['date','workid'])\n",
    "    \n",
    "    prediction_index = se_min.columns.get_loc(\"prediction\")\n",
    "    \n",
    "    for name, run in runs:\n",
    "        run = run.sort_values(\"actualArrival\")\n",
    "        \n",
    "        predict_array[run.index,:] = create_triangle(run.iloc[:, prediction_index])\n",
    "     \n",
    "    predict_array_cum = np.cumsum(predict_array, axis=1)\n",
    "    \n",
    "    first_20mins_mask = actual_array_cum < 20*60\n",
    "    \n",
    "    baseline_pass_count = np.count_nonzero((baseline_array_cum[first_20mins_mask] < actual_array_cum[first_20mins_mask] * 1.1) & \\\n",
    "                    (baseline_array_cum[first_20mins_mask] > actual_array_cum[first_20mins_mask] * 0.9))\n",
    "    \n",
    "    \n",
    "    pass_count = np.count_nonzero((predict_array_cum[first_20mins_mask] < actual_array_cum[first_20mins_mask] * 1.1) & \\\n",
    "                    (predict_array_cum[first_20mins_mask] > actual_array_cum[first_20mins_mask] * 0.9))\n",
    "    \n",
    "    pass_fraction = pass_count/np.count_nonzero(first_20mins_mask)\n",
    "        \n",
    "    print(f\"Approximately {(pass_count - baseline_pass_count)/baseline_pass_count*100:0.2f}% improvment on baseline\")\n",
    "    \n",
    "    print(f\"Approximately {pass_fraction*100:0.2f}% are in 10%\")\n",
    "    \n",
    "    return predict_array_cum, pass_count, pass_fraction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(data=[[1,2],[3,4],[5,6]], columns=['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['c'] = a['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "busses",
   "language": "python",
   "name": "busses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
